2024-03-05 22:39:11,443 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(569)) - starting cluster: numNameNodes=1, numDataNodes=2
2024-03-05 22:39:11,732 [main] WARN  util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(60)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2024-03-05 22:39:11,786 [main] INFO  namenode.NameNode (NameNode.java:format(1393)) - Formatting using clusterid: testClusterID
2024-03-05 22:39:11,796 [main] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(238)) - Edit logging is async:true
2024-03-05 22:39:11,808 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(869)) - KeyProvider: null
2024-03-05 22:39:11,809 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(142)) - fsLock is fair: true
2024-03-05 22:39:11,809 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(160)) - Detailed lock hold time metrics enabled: false
2024-03-05 22:39:11,813 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(904)) - fsOwner                = zhenyu (auth:SIMPLE)
2024-03-05 22:39:11,813 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(905)) - supergroup             = supergroup
2024-03-05 22:39:11,813 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(906)) - isPermissionEnabled    = true
2024-03-05 22:39:11,813 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(907)) - isStoragePolicyEnabled = true
2024-03-05 22:39:11,814 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(918)) - HA Enabled: false
2024-03-05 22:39:11,841 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(428)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2024-03-05 22:39:11,992 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:stopSlowPeerCollector(412)) - Slow peers collection thread shutdown
2024-03-05 22:39:11,994 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1462)) - hadoop.configured.node.mapping is deprecated. Instead, use net.topology.configured.node.mapping
2024-03-05 22:39:11,995 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:setBlockInvalidateLimit(2184)) - dfs.block.invalidate.limit : configured=1000, counted=60, effected=1000
2024-03-05 22:39:11,995 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(320)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2024-03-05 22:39:11,997 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(77)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2024-03-05 22:39:11,997 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(83)) - The block deletion will start around 2024 Mar 05 22:39:11
2024-03-05 22:39:11,998 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(408)) - Computing capacity for map BlocksMap
2024-03-05 22:39:11,999 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(409)) - VM type       = 64-bit
2024-03-05 22:39:11,999 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(410)) - 2.0% max memory 1.8 GB = 36.4 MB
2024-03-05 22:39:12,000 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(415)) - capacity      = 2^22 = 4194304 entries
2024-03-05 22:39:12,006 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createSPSManager(5786)) - Storage policy satisfier is disabled
2024-03-05 22:39:12,006 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(696)) - dfs.block.access.token.enable = false
2024-03-05 22:39:12,009 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(656)) - Using 1000 as SafeModeMonitor Interval
2024-03-05 22:39:12,010 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.999
2024-03-05 22:39:12,010 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2024-03-05 22:39:12,010 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2024-03-05 22:39:12,010 [main] INFO  blockmanagement.BlockManager (BlockManager.java:printInitialConfigs(682)) - defaultReplication         = 2
2024-03-05 22:39:12,010 [main] INFO  blockmanagement.BlockManager (BlockManager.java:printInitialConfigs(683)) - maxReplication             = 512
2024-03-05 22:39:12,010 [main] INFO  blockmanagement.BlockManager (BlockManager.java:printInitialConfigs(684)) - minReplication             = 1
2024-03-05 22:39:12,011 [main] INFO  blockmanagement.BlockManager (BlockManager.java:printInitialConfigs(685)) - maxReplicationStreams      = 2
2024-03-05 22:39:12,011 [main] INFO  blockmanagement.BlockManager (BlockManager.java:printInitialConfigs(686)) - redundancyRecheckInterval  = 3000ms
2024-03-05 22:39:12,011 [main] INFO  blockmanagement.BlockManager (BlockManager.java:printInitialConfigs(687)) - encryptDataTransfer        = false
2024-03-05 22:39:12,011 [main] INFO  blockmanagement.BlockManager (BlockManager.java:printInitialConfigs(688)) - maxNumBlocksToLog          = 1000
2024-03-05 22:39:12,147 [main] INFO  namenode.FSDirectory (SerialNumberManager.java:<clinit>(51)) - GLOBAL serial map: bits=29 maxEntries=536870911
2024-03-05 22:39:12,147 [main] INFO  namenode.FSDirectory (SerialNumberManager.java:<clinit>(51)) - USER serial map: bits=24 maxEntries=16777215
2024-03-05 22:39:12,147 [main] INFO  namenode.FSDirectory (SerialNumberManager.java:<clinit>(51)) - GROUP serial map: bits=24 maxEntries=16777215
2024-03-05 22:39:12,147 [main] INFO  namenode.FSDirectory (SerialNumberManager.java:<clinit>(51)) - XATTR serial map: bits=24 maxEntries=16777215
2024-03-05 22:39:12,154 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(408)) - Computing capacity for map INodeMap
2024-03-05 22:39:12,154 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(409)) - VM type       = 64-bit
2024-03-05 22:39:12,155 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(410)) - 1.0% max memory 1.8 GB = 18.2 MB
2024-03-05 22:39:12,155 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(415)) - capacity      = 2^21 = 2097152 entries
2024-03-05 22:39:12,156 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(339)) - ACLs enabled? true
2024-03-05 22:39:12,156 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(343)) - POSIX ACL inheritance enabled? true
2024-03-05 22:39:12,156 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(347)) - XAttrs enabled? true
2024-03-05 22:39:12,156 [main] INFO  namenode.NameNode (FSDirectory.java:<init>(414)) - Caching file names occurring more than 10 times
2024-03-05 22:39:12,159 [main] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(163)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotFSLimit: 65536, maxSnapshotLimit: 65536
2024-03-05 22:39:12,159 [main] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(176)) - dfs.namenode.snapshot.deletion.ordered = false
2024-03-05 22:39:12,160 [main] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2024-03-05 22:39:12,163 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(408)) - Computing capacity for map cachedBlocks
2024-03-05 22:39:12,163 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(409)) - VM type       = 64-bit
2024-03-05 22:39:12,163 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(410)) - 0.25% max memory 1.8 GB = 4.6 MB
2024-03-05 22:39:12,163 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(415)) - capacity      = 2^19 = 524288 entries
2024-03-05 22:39:12,168 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2024-03-05 22:39:12,168 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2024-03-05 22:39:12,168 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2024-03-05 22:39:12,170 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(1158)) - Retry cache on namenode is enabled
2024-03-05 22:39:12,170 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(1166)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2024-03-05 22:39:12,172 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(408)) - Computing capacity for map NameNodeRetryCache
2024-03-05 22:39:12,172 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(409)) - VM type       = 64-bit
2024-03-05 22:39:12,172 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(410)) - 0.029999999329447746% max memory 1.8 GB = 559.3 KB
2024-03-05 22:39:12,172 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(415)) - capacity      = 2^16 = 65536 entries
2024-03-05 22:39:12,187 [main] INFO  namenode.FSImage (FSImage.java:format(186)) - Allocated new BlockPoolId: BP-1607339391-127.0.1.1-1709696352182
2024-03-05 22:39:12,194 [main] INFO  common.Storage (NNStorage.java:format(595)) - Storage directory /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1 has been successfully formatted.
2024-03-05 22:39:12,195 [main] INFO  common.Storage (NNStorage.java:format(595)) - Storage directory /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2 has been successfully formatted.
2024-03-05 22:39:12,214 [FSImageSaver for /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(732)) - Saving image file /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage.ckpt_0000000000000000000 using no compression
2024-03-05 22:39:12,214 [FSImageSaver for /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(732)) - Saving image file /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/fsimage.ckpt_0000000000000000000 using no compression
2024-03-05 22:39:12,302 [FSImageSaver for /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(736)) - Image file /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage.ckpt_0000000000000000000 of size 401 bytes saved in 0 seconds .
2024-03-05 22:39:12,302 [FSImageSaver for /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(736)) - Image file /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/fsimage.ckpt_0000000000000000000 of size 401 bytes saved in 0 seconds .
2024-03-05 22:39:12,316 [main] INFO  namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:getImageTxIdToRetain(205)) - Going to retain 1 images with txid >= 0
2024-03-05 22:39:12,320 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:stopSlowPeerCollector(412)) - Slow peers collection thread shutdown
2024-03-05 22:39:12,359 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1509)) - Stopping services started for active state
2024-03-05 22:39:12,359 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1613)) - Stopping services started for standby state
2024-03-05 22:39:12,361 [main] INFO  namenode.NameNode (NameNode.java:createNameNode(1846)) - createNameNode []
2024-03-05 22:39:12,432 [main] INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(122)) - Loaded properties from hadoop-metrics2.properties
2024-03-05 22:39:12,474 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(378)) - Scheduled Metric snapshot period at 0 second(s).
2024-03-05 22:39:12,474 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2024-03-05 22:39:12,478 [main] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://127.0.0.1:0
2024-03-05 22:39:12,504 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@52b1beb6] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(185)) - Starting JVM pause monitor
2024-03-05 22:39:12,513 [main] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1769)) - Filter initializers set : org.apache.hadoop.http.lib.StaticUserWebFilter,org.apache.hadoop.hdfs.web.AuthFilterInitializer
2024-03-05 22:39:12,517 [main] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1791)) - Starting Web-server for hdfs at: http://localhost:0
2024-03-05 22:39:12,521 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1293)) - Web server is in development mode. Resources will be read from the source tree.
2024-03-05 22:39:12,529 [main] INFO  util.log (Log.java:initialized(170)) - Logging initialized @1489ms to org.eclipse.jetty.util.log.Slf4jLog
2024-03-05 22:39:12,609 [main] WARN  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /home/zhenyu/hadoop-http-auth-signature-secret
2024-03-05 22:39:12,630 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1293)) - Web server is in development mode. Resources will be read from the source tree.
2024-03-05 22:39:12,635 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(1205)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2024-03-05 22:39:12,636 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1178)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2024-03-05 22:39:12,636 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1188)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2024-03-05 22:39:12,636 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1188)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2024-03-05 22:39:12,638 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1178)) - Added filter AuthFilter (class=org.apache.hadoop.hdfs.web.AuthFilter) to context hdfs
2024-03-05 22:39:12,638 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1188)) - Added filter AuthFilter (class=org.apache.hadoop.hdfs.web.AuthFilter) to context static
2024-03-05 22:39:12,638 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1188)) - Added filter AuthFilter (class=org.apache.hadoop.hdfs.web.AuthFilter) to context logs
2024-03-05 22:39:12,640 [main] INFO  http.HttpServer2 (HttpServer2.java:addAsyncProfilerServlet(813)) - ASYNC_PROFILER_HOME environment variable and async.profiler.home system property not specified. Disabling /prof endpoint.
2024-03-05 22:39:12,664 [main] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(1032)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2024-03-05 22:39:12,669 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1452)) - Jetty bound to port 33163
2024-03-05 22:39:12,670 [main] INFO  server.Server (Server.java:doStart(375)) - jetty-9.4.53.v20231009; built: 2023-10-09T12:29:09.265Z; git: 27bde00a0b95a1d5bbee0eae7984f891d2d0f8c9; jvm 1.8.0_392-8u392-ga-1~22.04-b08
2024-03-05 22:39:12,690 [main] INFO  server.session (DefaultSessionIdManager.java:doStart(334)) - DefaultSessionIdManager workerName=node0
2024-03-05 22:39:12,690 [main] INFO  server.session (DefaultSessionIdManager.java:doStart(339)) - No SessionScavenger set, using defaults
2024-03-05 22:39:12,691 [main] INFO  server.session (HouseKeeper.java:startScavenging(132)) - node0 Scavenging every 660000ms
2024-03-05 22:39:12,702 [main] WARN  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /home/zhenyu/hadoop-http-auth-signature-secret
2024-03-05 22:39:12,704 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.s.ServletContextHandler@4bff7da0{logs,/logs,file:///home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2024-03-05 22:39:12,705 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.s.ServletContextHandler@536dbea0{static,/static,file:///home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2024-03-05 22:39:12,731 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.w.WebAppContext@6f7923a5{hdfs,/,file:///home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{file:/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs}
2024-03-05 22:39:12,739 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(333)) - Started ServerConnector@37911f88{HTTP/1.1, (http/1.1)}{localhost:33163}
2024-03-05 22:39:12,740 [main] INFO  server.Server (Server.java:doStart(415)) - Started @1699ms
2024-03-05 22:39:12,752 [main] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(238)) - Edit logging is async:true
2024-03-05 22:39:12,757 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(869)) - KeyProvider: null
2024-03-05 22:39:12,757 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(142)) - fsLock is fair: true
2024-03-05 22:39:12,757 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(160)) - Detailed lock hold time metrics enabled: false
2024-03-05 22:39:12,758 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(904)) - fsOwner                = zhenyu (auth:SIMPLE)
2024-03-05 22:39:12,758 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(905)) - supergroup             = supergroup
2024-03-05 22:39:12,758 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(906)) - isPermissionEnabled    = true
2024-03-05 22:39:12,758 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(907)) - isStoragePolicyEnabled = true
2024-03-05 22:39:12,758 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(918)) - HA Enabled: false
2024-03-05 22:39:12,758 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(428)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2024-03-05 22:39:12,758 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:stopSlowPeerCollector(412)) - Slow peers collection thread shutdown
2024-03-05 22:39:12,759 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:setBlockInvalidateLimit(2184)) - dfs.block.invalidate.limit : configured=1000, counted=60, effected=1000
2024-03-05 22:39:12,759 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(320)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2024-03-05 22:39:12,759 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(77)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2024-03-05 22:39:12,759 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(83)) - The block deletion will start around 2024 Mar 05 22:39:12
2024-03-05 22:39:12,759 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(408)) - Computing capacity for map BlocksMap
2024-03-05 22:39:12,759 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(409)) - VM type       = 64-bit
2024-03-05 22:39:12,759 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(410)) - 2.0% max memory 1.8 GB = 36.4 MB
2024-03-05 22:39:12,759 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(415)) - capacity      = 2^22 = 4194304 entries
2024-03-05 22:39:12,766 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createSPSManager(5786)) - Storage policy satisfier is disabled
2024-03-05 22:39:12,766 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(696)) - dfs.block.access.token.enable = false
2024-03-05 22:39:12,767 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(656)) - Using 1000 as SafeModeMonitor Interval
2024-03-05 22:39:12,767 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.999
2024-03-05 22:39:12,767 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2024-03-05 22:39:12,767 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2024-03-05 22:39:12,767 [main] INFO  blockmanagement.BlockManager (BlockManager.java:printInitialConfigs(682)) - defaultReplication         = 2
2024-03-05 22:39:12,767 [main] INFO  blockmanagement.BlockManager (BlockManager.java:printInitialConfigs(683)) - maxReplication             = 512
2024-03-05 22:39:12,767 [main] INFO  blockmanagement.BlockManager (BlockManager.java:printInitialConfigs(684)) - minReplication             = 1
2024-03-05 22:39:12,767 [main] INFO  blockmanagement.BlockManager (BlockManager.java:printInitialConfigs(685)) - maxReplicationStreams      = 2
2024-03-05 22:39:12,767 [main] INFO  blockmanagement.BlockManager (BlockManager.java:printInitialConfigs(686)) - redundancyRecheckInterval  = 3000ms
2024-03-05 22:39:12,767 [main] INFO  blockmanagement.BlockManager (BlockManager.java:printInitialConfigs(687)) - encryptDataTransfer        = false
2024-03-05 22:39:12,767 [main] INFO  blockmanagement.BlockManager (BlockManager.java:printInitialConfigs(688)) - maxNumBlocksToLog          = 1000
2024-03-05 22:39:12,768 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(408)) - Computing capacity for map INodeMap
2024-03-05 22:39:12,768 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(409)) - VM type       = 64-bit
2024-03-05 22:39:12,768 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(410)) - 1.0% max memory 1.8 GB = 18.2 MB
2024-03-05 22:39:12,768 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(415)) - capacity      = 2^21 = 2097152 entries
2024-03-05 22:39:12,771 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(339)) - ACLs enabled? true
2024-03-05 22:39:12,771 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(343)) - POSIX ACL inheritance enabled? true
2024-03-05 22:39:12,771 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(347)) - XAttrs enabled? true
2024-03-05 22:39:12,771 [main] INFO  namenode.NameNode (FSDirectory.java:<init>(414)) - Caching file names occurring more than 10 times
2024-03-05 22:39:12,772 [main] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(163)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotFSLimit: 65536, maxSnapshotLimit: 65536
2024-03-05 22:39:12,772 [main] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(176)) - dfs.namenode.snapshot.deletion.ordered = false
2024-03-05 22:39:12,772 [main] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2024-03-05 22:39:12,772 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(408)) - Computing capacity for map cachedBlocks
2024-03-05 22:39:12,772 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(409)) - VM type       = 64-bit
2024-03-05 22:39:12,772 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(410)) - 0.25% max memory 1.8 GB = 4.6 MB
2024-03-05 22:39:12,772 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(415)) - capacity      = 2^19 = 524288 entries
2024-03-05 22:39:12,773 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2024-03-05 22:39:12,773 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2024-03-05 22:39:12,773 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2024-03-05 22:39:12,773 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(1158)) - Retry cache on namenode is enabled
2024-03-05 22:39:12,773 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(1166)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2024-03-05 22:39:12,774 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(408)) - Computing capacity for map NameNodeRetryCache
2024-03-05 22:39:12,774 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(409)) - VM type       = 64-bit
2024-03-05 22:39:12,774 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(410)) - 0.029999999329447746% max memory 1.8 GB = 559.3 KB
2024-03-05 22:39:12,774 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(415)) - capacity      = 2^16 = 65536 entries
2024-03-05 22:39:12,776 [main] INFO  common.Storage (Storage.java:tryLock(948)) - Lock on /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/in_use.lock acquired by nodename 4102809@lift02
2024-03-05 22:39:12,778 [main] INFO  common.Storage (Storage.java:tryLock(948)) - Lock on /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/in_use.lock acquired by nodename 4102809@lift02
2024-03-05 22:39:12,779 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(428)) - Recovering unfinalized segments in /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current
2024-03-05 22:39:12,779 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(428)) - Recovering unfinalized segments in /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current
2024-03-05 22:39:12,779 [main] INFO  namenode.FSImage (FSImage.java:loadFSImage(734)) - No edit log streams selected.
2024-03-05 22:39:12,779 [main] INFO  namenode.FSImage (FSImage.java:loadFSImageFile(800)) - Planning to load image: FSImageFile(file=/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2024-03-05 22:39:12,793 [main] INFO  namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSectionHeader(411)) - Loading 1 INodes.
2024-03-05 22:39:12,793 [main] INFO  namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(369)) - Successfully loaded 1 inodes
2024-03-05 22:39:12,796 [main] INFO  namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:waitBlocksMapAndNameCacheUpdateFinished(342)) - Completed update blocks map and name cache, total waiting duration 0ms.
2024-03-05 22:39:12,797 [main] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(255)) - Loaded FSImage in 0 seconds.
2024-03-05 22:39:12,797 [main] INFO  namenode.FSImage (FSImage.java:loadFSImage(980)) - Loaded image for txid 0 from /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000000
2024-03-05 22:39:12,800 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1280)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2024-03-05 22:39:12,800 [main] INFO  namenode.FSEditLog (FSEditLog.java:startLogSegment(1417)) - Starting log segment at 1
2024-03-05 22:39:12,809 [main] INFO  namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2024-03-05 22:39:12,809 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(841)) - Finished loading FSImage in 34 msecs
2024-03-05 22:39:13,030 [main] INFO  namenode.NameNode (NameNodeRpcServer.java:<init>(452)) - RPC server is binding to localhost:0
2024-03-05 22:39:13,030 [main] INFO  namenode.NameNode (NameNodeRpcServer.java:<init>(457)) - Enable NameNode state context:false
2024-03-05 22:39:13,036 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(93)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false, ipcFailOver: false.
2024-03-05 22:39:13,043 [main] INFO  ipc.Server (Server.java:<init>(1440)) - Listener at localhost:36175
2024-03-05 22:39:13,044 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1476)) - Starting Socket Reader #1 for port 0
2024-03-05 22:39:13,075 [main] INFO  namenode.NameNode (NameNode.java:initialize(916)) - Clients are to use localhost:36175 to access this namenode/service.
2024-03-05 22:39:13,077 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:registerMBean(5661)) - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2024-03-05 22:39:13,088 [main] INFO  namenode.LeaseManager (LeaseManager.java:getNumUnderConstructionBlocks(166)) - Number of blocks under construction: 0
2024-03-05 22:39:13,093 [main] INFO  blockmanagement.DatanodeAdminDefaultMonitor (DatanodeAdminDefaultMonitor.java:processConf(126)) - Initialized the Default Decommission and Maintenance monitor
2024-03-05 22:39:13,094 [MarkedDeleteBlockScrubberThread] INFO  blockmanagement.BlockManager (BlockManager.java:run(5300)) - Start MarkedDeleteBlockScrubber thread
2024-03-05 22:39:13,095 [main] INFO  blockmanagement.BlockManager (BlockManager.java:initializeReplQueues(5513)) - initializing replication queues
2024-03-05 22:39:13,095 [main] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(409)) - STATE* Leaving safe mode after 0 secs
2024-03-05 22:39:13,095 [main] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(415)) - STATE* Network topology has 0 racks and 0 datanodes
2024-03-05 22:39:13,095 [main] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(417)) - STATE* UnderReplicatedBlocks has 0 blocks
2024-03-05 22:39:13,100 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(4034)) - Total number of blocks            = 0
2024-03-05 22:39:13,100 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(4035)) - Number of invalid blocks          = 0
2024-03-05 22:39:13,100 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(4036)) - Number of under-replicated blocks = 0
2024-03-05 22:39:13,100 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(4037)) - Number of  over-replicated blocks = 0
2024-03-05 22:39:13,100 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(4039)) - Number of blocks being written    = 0
2024-03-05 22:39:13,100 [Reconstruction Queue Initializer] INFO  hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(4042)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 5 msec
2024-03-05 22:39:13,100 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(4051)) - Reconstruction queues initialisation progress: 0.0, total number of blocks processed: 0/0
2024-03-05 22:39:13,114 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1715)) - IPC Server Responder: starting
2024-03-05 22:39:13,115 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1555)) - IPC Server listener on 0: starting
2024-03-05 22:39:13,116 [main] INFO  namenode.NameNode (NameNode.java:startCommonServices(1031)) - NameNode RPC up at: localhost/127.0.0.1:36175.
2024-03-05 22:39:13,117 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1392)) - Starting services required for active state
2024-03-05 22:39:13,117 [main] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(857)) - Initializing quota with 12 thread(s)
2024-03-05 22:39:13,121 [main] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(866)) - Quota initialization completed in 4 milliseconds
name space=1
storage space=0
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0, NVDIMM=0
2024-03-05 22:39:13,123 [CacheReplicationMonitor(378060006)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(165)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2024-03-05 22:39:13,127 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1772)) - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1,[DISK]file:/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
secureResources isnull
2024-03-05 22:39:13,159 [main] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2024-03-05 22:39:13,167 [main] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2024-03-05 22:39:13,197 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2024-03-05 22:39:13,200 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(428)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2024-03-05 22:39:13,202 [main] INFO  datanode.BlockScanner (BlockScanner.java:<init>(201)) - Initialized block scanner with targetBytesPerSec 1048576
2024-03-05 22:39:13,205 [main] INFO  datanode.DataNode (DataNode.java:<init>(595)) - Configured hostname is 127.0.0.1
2024-03-05 22:39:13,206 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(428)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2024-03-05 22:39:13,208 [main] INFO  datanode.DataNode (DataNode.java:startDataNode(1932)) - Starting DataNode with maxLockedMemory = 0
2024-03-05 22:39:13,211 [main] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1668)) - secureResources isnull
2024-03-05 22:39:13,211 [main] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1675)) - Socket Write Timeout is 480000
2024-03-05 22:39:13,212 [main] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1678)) - Failure Recovery DataNode /127.0.0.1:0
2024-03-05 22:39:13,212 [main] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1681)) - TcpPeerServer created at /127.0.0.1:0
2024-03-05 22:39:13,212 [main] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1690)) - Opened streaming server at /127.0.0.1:36261
2024-03-05 22:39:13,212 [main] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1691)) - Opened shadow Streaming server at /127.0.0.1:45427
2024-03-05 22:39:13,214 [main] INFO  datanode.DataNode (DataXceiverServer.java:<init>(94)) - Balancing bandwidth is 104857600 bytes/s
2024-03-05 22:39:13,214 [main] INFO  datanode.DataNode (DataXceiverServer.java:<init>(95)) - Number threads for balancing is 100
2024-03-05 22:39:13,214 [main] INFO  datanode.DataNode (DataXceiverServer.java:<init>(94)) - Balancing bandwidth is 104857600 bytes/s
2024-03-05 22:39:13,214 [main] INFO  datanode.DataNode (DataXceiverServer.java:<init>(95)) - Number threads for balancing is 100
2024-03-05 22:39:13,218 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1293)) - Web server is in development mode. Resources will be read from the source tree.
2024-03-05 22:39:13,219 [main] WARN  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /home/zhenyu/hadoop-http-auth-signature-secret
2024-03-05 22:39:13,221 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1293)) - Web server is in development mode. Resources will be read from the source tree.
2024-03-05 22:39:13,222 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(1205)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2024-03-05 22:39:13,223 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1178)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2024-03-05 22:39:13,223 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1188)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2024-03-05 22:39:13,223 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1188)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2024-03-05 22:39:13,223 [main] INFO  http.HttpServer2 (HttpServer2.java:addAsyncProfilerServlet(813)) - ASYNC_PROFILER_HOME environment variable and async.profiler.home system property not specified. Disabling /prof endpoint.
2024-03-05 22:39:13,225 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1452)) - Jetty bound to port 43565
2024-03-05 22:39:13,225 [main] INFO  server.Server (Server.java:doStart(375)) - jetty-9.4.53.v20231009; built: 2023-10-09T12:29:09.265Z; git: 27bde00a0b95a1d5bbee0eae7984f891d2d0f8c9; jvm 1.8.0_392-8u392-ga-1~22.04-b08
2024-03-05 22:39:13,226 [main] INFO  server.session (DefaultSessionIdManager.java:doStart(334)) - DefaultSessionIdManager workerName=node0
2024-03-05 22:39:13,226 [main] INFO  server.session (DefaultSessionIdManager.java:doStart(339)) - No SessionScavenger set, using defaults
2024-03-05 22:39:13,226 [main] INFO  server.session (HouseKeeper.java:startScavenging(132)) - node0 Scavenging every 660000ms
2024-03-05 22:39:13,227 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.s.ServletContextHandler@503ecb24{logs,/logs,file:///home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2024-03-05 22:39:13,227 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.s.ServletContextHandler@6995bf68{static,/static,file:///home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2024-03-05 22:39:13,230 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.w.WebAppContext@4b14918a{datanode,/,file:///home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{file:/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode}
2024-03-05 22:39:13,231 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(333)) - Started ServerConnector@4b1d6571{HTTP/1.1, (http/1.1)}{localhost:43565}
2024-03-05 22:39:13,231 [main] INFO  server.Server (Server.java:doStart(415)) - Started @2190ms
2024-03-05 22:39:13,331 [main] WARN  web.DatanodeHttpServer (RestCsrfPreventionFilterHandler.java:<init>(75)) - Got null for restCsrfPreventionFilter - will not do any filtering.
2024-03-05 22:39:13,398 [main] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(306)) - Listening HTTP traffic on /127.0.0.1:38381
2024-03-05 22:39:13,399 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@285c08c8] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(185)) - Starting JVM pause monitor
2024-03-05 22:39:13,399 [main] INFO  datanode.DataNode (DataNode.java:startDataNode(1960)) - dnUserName = zhenyu
2024-03-05 22:39:13,399 [main] INFO  datanode.DataNode (DataNode.java:startDataNode(1961)) - supergroup = supergroup
2024-03-05 22:39:13,408 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(93)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false, ipcFailOver: false.
2024-03-05 22:39:13,409 [main] INFO  ipc.Server (Server.java:<init>(1440)) - Listener at localhost:37983
2024-03-05 22:39:13,409 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1476)) - Starting Socket Reader #1 for port 0
2024-03-05 22:39:13,412 [main] INFO  datanode.DataNode (DataNode.java:initIpcServer(1568)) - Opened IPC server at /127.0.0.1:37983
2024-03-05 22:39:13,425 [main] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(150)) - Refresh request received for nameservices: null
2024-03-05 22:39:13,425 [main] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(211)) - Starting BPOfferServices for nameservices: <default>
2024-03-05 22:39:13,433 [Thread-75] INFO  datanode.DataNode (BPServiceActor.java:run(886)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:36175 starting to offer service
2024-03-05 22:39:13,435 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1715)) - IPC Server Responder: starting
2024-03-05 22:39:13,435 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1555)) - IPC Server listener on 0: starting
2024-03-05 22:39:13,437 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1772)) - Starting DataNode 1 with dfs.datanode.data.dir: [DISK]file:/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3,[DISK]file:/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
secureResources isnull
2024-03-05 22:39:13,438 [main] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
2024-03-05 22:39:13,438 [main] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2024-03-05 22:39:13,447 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2024-03-05 22:39:13,447 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(428)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2024-03-05 22:39:13,447 [main] INFO  datanode.BlockScanner (BlockScanner.java:<init>(201)) - Initialized block scanner with targetBytesPerSec 1048576
2024-03-05 22:39:13,448 [main] INFO  datanode.DataNode (DataNode.java:<init>(595)) - Configured hostname is 127.0.0.1
2024-03-05 22:39:13,448 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(428)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2024-03-05 22:39:13,448 [main] INFO  datanode.DataNode (DataNode.java:startDataNode(1932)) - Starting DataNode with maxLockedMemory = 0
2024-03-05 22:39:13,448 [main] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1668)) - secureResources isnull
2024-03-05 22:39:13,448 [main] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1675)) - Socket Write Timeout is 480000
2024-03-05 22:39:13,449 [main] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1678)) - Failure Recovery DataNode /127.0.0.1:0
2024-03-05 22:39:13,449 [main] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1681)) - TcpPeerServer created at /127.0.0.1:0
2024-03-05 22:39:13,449 [main] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1690)) - Opened streaming server at /127.0.0.1:39695
2024-03-05 22:39:13,449 [main] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1691)) - Opened shadow Streaming server at /127.0.0.1:39543
2024-03-05 22:39:13,449 [main] INFO  datanode.DataNode (DataXceiverServer.java:<init>(94)) - Balancing bandwidth is 104857600 bytes/s
2024-03-05 22:39:13,449 [main] INFO  datanode.DataNode (DataXceiverServer.java:<init>(95)) - Number threads for balancing is 100
2024-03-05 22:39:13,449 [main] INFO  datanode.DataNode (DataXceiverServer.java:<init>(94)) - Balancing bandwidth is 104857600 bytes/s
2024-03-05 22:39:13,449 [main] INFO  datanode.DataNode (DataXceiverServer.java:<init>(95)) - Number threads for balancing is 100
2024-03-05 22:39:13,450 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1293)) - Web server is in development mode. Resources will be read from the source tree.
2024-03-05 22:39:13,452 [main] WARN  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /home/zhenyu/hadoop-http-auth-signature-secret
2024-03-05 22:39:13,454 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1293)) - Web server is in development mode. Resources will be read from the source tree.
2024-03-05 22:39:13,455 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(1205)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2024-03-05 22:39:13,455 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1178)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2024-03-05 22:39:13,455 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1188)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2024-03-05 22:39:13,456 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(1188)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2024-03-05 22:39:13,456 [main] INFO  http.HttpServer2 (HttpServer2.java:addAsyncProfilerServlet(813)) - ASYNC_PROFILER_HOME environment variable and async.profiler.home system property not specified. Disabling /prof endpoint.
2024-03-05 22:39:13,456 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1452)) - Jetty bound to port 41691
2024-03-05 22:39:13,456 [main] INFO  server.Server (Server.java:doStart(375)) - jetty-9.4.53.v20231009; built: 2023-10-09T12:29:09.265Z; git: 27bde00a0b95a1d5bbee0eae7984f891d2d0f8c9; jvm 1.8.0_392-8u392-ga-1~22.04-b08
2024-03-05 22:39:13,457 [main] INFO  server.session (DefaultSessionIdManager.java:doStart(334)) - DefaultSessionIdManager workerName=node0
2024-03-05 22:39:13,457 [main] INFO  server.session (DefaultSessionIdManager.java:doStart(339)) - No SessionScavenger set, using defaults
2024-03-05 22:39:13,457 [main] INFO  server.session (HouseKeeper.java:startScavenging(132)) - node0 Scavenging every 600000ms
2024-03-05 22:39:13,458 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.s.ServletContextHandler@4bff1903{logs,/logs,file:///home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2024-03-05 22:39:13,458 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.s.ServletContextHandler@5827af16{static,/static,file:///home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2024-03-05 22:39:13,462 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(921)) - Started o.e.j.w.WebAppContext@71b3bc45{datanode,/,file:///home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{file:/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode}
2024-03-05 22:39:13,462 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(333)) - Started ServerConnector@239b0f9d{HTTP/1.1, (http/1.1)}{localhost:41691}
2024-03-05 22:39:13,462 [main] INFO  server.Server (Server.java:doStart(415)) - Started @2422ms
2024-03-05 22:39:13,512 [main] WARN  web.DatanodeHttpServer (RestCsrfPreventionFilterHandler.java:<init>(75)) - Got null for restCsrfPreventionFilter - will not do any filtering.
2024-03-05 22:39:13,513 [main] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(306)) - Listening HTTP traffic on /127.0.0.1:38133
2024-03-05 22:39:13,513 [main] INFO  datanode.DataNode (DataNode.java:startDataNode(1960)) - dnUserName = zhenyu
2024-03-05 22:39:13,513 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@6e4de19b] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(185)) - Starting JVM pause monitor
2024-03-05 22:39:13,513 [main] INFO  datanode.DataNode (DataNode.java:startDataNode(1961)) - supergroup = supergroup
2024-03-05 22:39:13,514 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(93)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false, ipcFailOver: false.
2024-03-05 22:39:13,514 [main] INFO  ipc.Server (Server.java:<init>(1440)) - Listener at localhost:35077
2024-03-05 22:39:13,514 [Socket Reader #1 for port 0] INFO  ipc.Server (Server.java:run(1476)) - Starting Socket Reader #1 for port 0
2024-03-05 22:39:13,516 [main] INFO  datanode.DataNode (DataNode.java:initIpcServer(1568)) - Opened IPC server at /127.0.0.1:35077
2024-03-05 22:39:13,524 [main] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(150)) - Refresh request received for nameservices: null
2024-03-05 22:39:13,525 [main] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(211)) - Starting BPOfferServices for nameservices: <default>
2024-03-05 22:39:13,525 [Thread-104] INFO  datanode.DataNode (BPServiceActor.java:run(886)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:36175 starting to offer service
2024-03-05 22:39:13,526 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1555)) - IPC Server listener on 0: starting
2024-03-05 22:39:13,526 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1715)) - IPC Server Responder: starting
2024-03-05 22:39:13,595 [Thread-75] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(381)) - Acknowledging ACTIVE Namenode during handshake Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:36175
2024-03-05 22:39:13,595 [Thread-104] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(381)) - Acknowledging ACTIVE Namenode during handshake Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:36175
2024-03-05 22:39:13,596 [Thread-75] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(356)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2024-03-05 22:39:13,596 [Thread-104] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(356)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2024-03-05 22:39:13,597 [Thread-75] INFO  common.Storage (Storage.java:tryLock(948)) - Lock on /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/in_use.lock acquired by nodename 4102809@lift02
2024-03-05 22:39:13,597 [Thread-104] INFO  common.Storage (Storage.java:tryLock(948)) - Lock on /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/in_use.lock acquired by nodename 4102809@lift02
2024-03-05 22:39:13,598 [Thread-75] INFO  common.Storage (DataStorage.java:loadStorageDirectory(284)) - Storage directory with location [DISK]file:/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1 is not formatted for namespace 783508685. Formatting...
2024-03-05 22:39:13,598 [Thread-104] INFO  common.Storage (DataStorage.java:loadStorageDirectory(284)) - Storage directory with location [DISK]file:/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3 is not formatted for namespace 783508685. Formatting...
2024-03-05 22:39:13,600 [Thread-75] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-fb698f9f-eda6-4ec4-8154-78ab02d10859 for directory /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1 
2024-03-05 22:39:13,600 [Thread-104] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-19547c29-546b-4b55-86e8-03fd99bb066e for directory /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3 
2024-03-05 22:39:13,601 [Thread-75] INFO  common.Storage (Storage.java:tryLock(948)) - Lock on /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/in_use.lock acquired by nodename 4102809@lift02
2024-03-05 22:39:13,601 [Thread-75] INFO  common.Storage (DataStorage.java:loadStorageDirectory(284)) - Storage directory with location [DISK]file:/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2 is not formatted for namespace 783508685. Formatting...
2024-03-05 22:39:13,601 [Thread-75] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-a0953621-3236-40ef-8e69-cbc77fe0dd76 for directory /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2 
2024-03-05 22:39:13,602 [Thread-104] INFO  common.Storage (Storage.java:tryLock(948)) - Lock on /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/in_use.lock acquired by nodename 4102809@lift02
2024-03-05 22:39:13,602 [Thread-104] INFO  common.Storage (DataStorage.java:loadStorageDirectory(284)) - Storage directory with location [DISK]file:/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4 is not formatted for namespace 783508685. Formatting...
2024-03-05 22:39:13,603 [Thread-104] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-dbb30921-c74f-4995-8ef2-b17475f34882 for directory /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4 
2024-03-05 22:39:13,617 [Thread-75] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(255)) - Analyzing storage directories for bpid BP-1607339391-127.0.1.1-1709696352182
2024-03-05 22:39:13,617 [Thread-75] INFO  common.Storage (Storage.java:lock(907)) - Locking is disabled for /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1607339391-127.0.1.1-1709696352182
2024-03-05 22:39:13,618 [Thread-75] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1 and block pool id BP-1607339391-127.0.1.1-1709696352182 is not formatted. Formatting ...
2024-03-05 22:39:13,618 [Thread-75] INFO  common.Storage (BlockPoolSliceStorage.java:format(284)) - Formatting block pool BP-1607339391-127.0.1.1-1709696352182 directory /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1607339391-127.0.1.1-1709696352182/current
2024-03-05 22:39:13,624 [Thread-104] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(255)) - Analyzing storage directories for bpid BP-1607339391-127.0.1.1-1709696352182
2024-03-05 22:39:13,624 [Thread-104] INFO  common.Storage (Storage.java:lock(907)) - Locking is disabled for /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1607339391-127.0.1.1-1709696352182
2024-03-05 22:39:13,624 [Thread-104] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3 and block pool id BP-1607339391-127.0.1.1-1709696352182 is not formatted. Formatting ...
2024-03-05 22:39:13,624 [Thread-104] INFO  common.Storage (BlockPoolSliceStorage.java:format(284)) - Formatting block pool BP-1607339391-127.0.1.1-1709696352182 directory /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1607339391-127.0.1.1-1709696352182/current
2024-03-05 22:39:13,632 [Thread-75] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(255)) - Analyzing storage directories for bpid BP-1607339391-127.0.1.1-1709696352182
2024-03-05 22:39:13,633 [Thread-75] INFO  common.Storage (Storage.java:lock(907)) - Locking is disabled for /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1607339391-127.0.1.1-1709696352182
2024-03-05 22:39:13,633 [Thread-75] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2 and block pool id BP-1607339391-127.0.1.1-1709696352182 is not formatted. Formatting ...
2024-03-05 22:39:13,633 [Thread-75] INFO  common.Storage (BlockPoolSliceStorage.java:format(284)) - Formatting block pool BP-1607339391-127.0.1.1-1709696352182 directory /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1607339391-127.0.1.1-1709696352182/current
2024-03-05 22:39:13,634 [Thread-75] INFO  datanode.DataNode (DataNode.java:initStorage(2280)) - Setting up storage: nsid=783508685;bpid=BP-1607339391-127.0.1.1-1709696352182;lv=-57;nsInfo=lv=-67;cid=testClusterID;nsid=783508685;c=1709696352182;bpid=BP-1607339391-127.0.1.1-1709696352182;dnuuid=null
2024-03-05 22:39:13,635 [Thread-75] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(2078)) - Generated and persisted new Datanode UUID e4a85aef-13ce-4b47-8b3c-6054f58e20ea
2024-03-05 22:39:13,640 [Thread-104] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(255)) - Analyzing storage directories for bpid BP-1607339391-127.0.1.1-1709696352182
2024-03-05 22:39:13,640 [Thread-104] INFO  common.Storage (Storage.java:lock(907)) - Locking is disabled for /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1607339391-127.0.1.1-1709696352182
2024-03-05 22:39:13,640 [Thread-104] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4 and block pool id BP-1607339391-127.0.1.1-1709696352182 is not formatted. Formatting ...
2024-03-05 22:39:13,641 [Thread-104] INFO  common.Storage (BlockPoolSliceStorage.java:format(284)) - Formatting block pool BP-1607339391-127.0.1.1-1709696352182 directory /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1607339391-127.0.1.1-1709696352182/current
2024-03-05 22:39:13,642 [Thread-104] INFO  datanode.DataNode (DataNode.java:initStorage(2280)) - Setting up storage: nsid=783508685;bpid=BP-1607339391-127.0.1.1-1709696352182;lv=-57;nsInfo=lv=-67;cid=testClusterID;nsid=783508685;c=1709696352182;bpid=BP-1607339391-127.0.1.1-1709696352182;dnuuid=null
2024-03-05 22:39:13,642 [Thread-104] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(2078)) - Generated and persisted new Datanode UUID e0db7eb1-f861-4de4-a0df-2a2a3acfcfcd
2024-03-05 22:39:13,660 [Thread-75] INFO  fsdataset.RoundRobinVolumeChoosingPolicy (RoundRobinVolumeChoosingPolicy.java:setConf(67)) - Round robin volume choosing policy initialized: dfs.datanode.round-robin-volume-choosing-policy.additional-available-space = 0
2024-03-05 22:39:13,660 [Thread-104] INFO  fsdataset.RoundRobinVolumeChoosingPolicy (RoundRobinVolumeChoosingPolicy.java:setConf(67)) - Round robin volume choosing policy initialized: dfs.datanode.round-robin-volume-choosing-policy.additional-available-space = 0
2024-03-05 22:39:13,758 [Thread-104] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(400)) - Added new volume: DS-19547c29-546b-4b55-86e8-03fd99bb066e
2024-03-05 22:39:13,758 [Thread-75] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(400)) - Added new volume: DS-fb698f9f-eda6-4ec4-8154-78ab02d10859
2024-03-05 22:39:13,758 [Thread-104] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(483)) - Added volume - [DISK]file:/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3, StorageType: DISK
2024-03-05 22:39:13,758 [Thread-75] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(483)) - Added volume - [DISK]file:/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, StorageType: DISK
2024-03-05 22:39:13,759 [Thread-104] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(400)) - Added new volume: DS-dbb30921-c74f-4995-8ef2-b17475f34882
2024-03-05 22:39:13,759 [Thread-104] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(483)) - Added volume - [DISK]file:/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, StorageType: DISK
2024-03-05 22:39:13,761 [Thread-75] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(400)) - Added new volume: DS-a0953621-3236-40ef-8e69-cbc77fe0dd76
2024-03-05 22:39:13,761 [Thread-75] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(483)) - Added volume - [DISK]file:/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, StorageType: DISK
2024-03-05 22:39:13,762 [Thread-104] INFO  impl.MemoryMappableBlockLoader (MemoryMappableBlockLoader.java:initialize(47)) - Initializing cache loader: MemoryMappableBlockLoader.
2024-03-05 22:39:13,762 [Thread-75] INFO  impl.MemoryMappableBlockLoader (MemoryMappableBlockLoader.java:initialize(47)) - Initializing cache loader: MemoryMappableBlockLoader.
2024-03-05 22:39:13,764 [Thread-104] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2637)) - Registered FSDatasetState MBean
2024-03-05 22:39:13,764 [Thread-75] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2637)) - Registered FSDatasetState MBean
2024-03-05 22:39:13,766 [Thread-104] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(3219)) - Adding block pool BP-1607339391-127.0.1.1-1709696352182
2024-03-05 22:39:13,767 [Thread-126] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(513)) - Scanning block pool BP-1607339391-127.0.1.1-1709696352182 on volume /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4...
2024-03-05 22:39:13,767 [Thread-125] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(513)) - Scanning block pool BP-1607339391-127.0.1.1-1709696352182 on volume /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3...
2024-03-05 22:39:13,768 [Thread-75] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(3219)) - Adding block pool BP-1607339391-127.0.1.1-1709696352182
2024-03-05 22:39:13,768 [Thread-127] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(513)) - Scanning block pool BP-1607339391-127.0.1.1-1709696352182 on volume /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2024-03-05 22:39:13,769 [Thread-128] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(513)) - Scanning block pool BP-1607339391-127.0.1.1-1709696352182 on volume /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2024-03-05 22:39:13,772 [Thread-128] WARN  impl.FsDatasetImpl (BlockPoolSlice.java:loadDfsUsed(347)) - dfsUsed file missing in /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1607339391-127.0.1.1-1709696352182/current, will proceed with Du for space computation calculation, 
2024-03-05 22:39:13,772 [Thread-126] WARN  impl.FsDatasetImpl (BlockPoolSlice.java:loadDfsUsed(347)) - dfsUsed file missing in /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1607339391-127.0.1.1-1709696352182/current, will proceed with Du for space computation calculation, 
2024-03-05 22:39:13,772 [Thread-127] WARN  impl.FsDatasetImpl (BlockPoolSlice.java:loadDfsUsed(347)) - dfsUsed file missing in /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1607339391-127.0.1.1-1709696352182/current, will proceed with Du for space computation calculation, 
2024-03-05 22:39:13,772 [Thread-125] WARN  impl.FsDatasetImpl (BlockPoolSlice.java:loadDfsUsed(347)) - dfsUsed file missing in /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1607339391-127.0.1.1-1709696352182/current, will proceed with Du for space computation calculation, 
2024-03-05 22:39:13,783 [IPC Server handler 2 on default port 36175] INFO  lib.Interns (Interns.java:removeEldestEntry(50)) - Metrics intern cache overflow at 2011 for MetricsSystem={MetricsSystem=MetricsInfoImpl{name=MetricsSystem, description=MetricsSystem}, MetricsSystem record=MetricsInfoImpl{name=MetricsSystem, description=MetricsSystem record}}
2024-03-05 22:39:13,786 [Thread-126] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(518)) - Time taken to scan block pool BP-1607339391-127.0.1.1-1709696352182 on /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4: 19ms
2024-03-05 22:39:13,788 [Thread-125] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(518)) - Time taken to scan block pool BP-1607339391-127.0.1.1-1709696352182 on /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3: 21ms
2024-03-05 22:39:13,788 [Thread-104] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(538)) - Total time to scan all replicas for block pool BP-1607339391-127.0.1.1-1709696352182: 22ms
2024-03-05 22:39:13,788 [Thread-128] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(518)) - Time taken to scan block pool BP-1607339391-127.0.1.1-1709696352182 on /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 20ms
2024-03-05 22:39:13,788 [Thread-127] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(518)) - Time taken to scan block pool BP-1607339391-127.0.1.1-1709696352182 on /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 20ms
2024-03-05 22:39:13,789 [Thread-75] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(538)) - Total time to scan all replicas for block pool BP-1607339391-127.0.1.1-1709696352182: 21ms
2024-03-05 22:39:13,789 [Thread-133] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(266)) - Adding replicas to map for block pool BP-1607339391-127.0.1.1-1709696352182 on volume /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3...
2024-03-05 22:39:13,789 [Thread-134] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(266)) - Adding replicas to map for block pool BP-1607339391-127.0.1.1-1709696352182 on volume /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2024-03-05 22:39:13,789 [Thread-136] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(266)) - Adding replicas to map for block pool BP-1607339391-127.0.1.1-1709696352182 on volume /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2024-03-05 22:39:13,789 [Thread-135] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(266)) - Adding replicas to map for block pool BP-1607339391-127.0.1.1-1709696352182 on volume /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4...
2024-03-05 22:39:13,790 [Thread-136] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(963)) - Replica Cache file: /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1607339391-127.0.1.1-1709696352182/current/replicas doesn't exist 
2024-03-05 22:39:13,790 [Thread-135] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(963)) - Replica Cache file: /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1607339391-127.0.1.1-1709696352182/current/replicas doesn't exist 
2024-03-05 22:39:13,789 [Thread-133] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(963)) - Replica Cache file: /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1607339391-127.0.1.1-1709696352182/current/replicas doesn't exist 
2024-03-05 22:39:13,789 [Thread-134] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(963)) - Replica Cache file: /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1607339391-127.0.1.1-1709696352182/current/replicas doesn't exist 
2024-03-05 22:39:13,790 [Thread-136] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(271)) - Time to add replicas to map for block pool BP-1607339391-127.0.1.1-1709696352182 on volume /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 0ms
2024-03-05 22:39:13,790 [Thread-134] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(271)) - Time to add replicas to map for block pool BP-1607339391-127.0.1.1-1709696352182 on volume /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 0ms
2024-03-05 22:39:13,790 [Thread-133] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(271)) - Time to add replicas to map for block pool BP-1607339391-127.0.1.1-1709696352182 on volume /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3: 0ms
2024-03-05 22:39:13,790 [Thread-75] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(292)) - Total time to add all replicas to map for block pool BP-1607339391-127.0.1.1-1709696352182: 2ms
2024-03-05 22:39:13,790 [Thread-135] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(271)) - Time to add replicas to map for block pool BP-1607339391-127.0.1.1-1709696352182 on volume /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4: 0ms
2024-03-05 22:39:13,791 [Thread-104] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(292)) - Total time to add all replicas to map for block pool BP-1607339391-127.0.1.1-1709696352182: 2ms
2024-03-05 22:39:13,791 [Thread-75] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2024-03-05 22:39:13,791 [Thread-104] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
2024-03-05 22:39:13,794 [Thread-75] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(223)) - Scheduled health check for volume /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2024-03-05 22:39:13,794 [Thread-104] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(223)) - Scheduled health check for volume /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
2024-03-05 22:39:13,795 [Thread-104] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2024-03-05 22:39:13,795 [Thread-75] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2024-03-05 22:39:13,795 [Thread-104] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(223)) - Scheduled health check for volume /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2024-03-05 22:39:13,795 [Thread-75] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(223)) - Scheduled health check for volume /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2024-03-05 22:39:13,796 [VolumeScannerThread(/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(385)) - Now scanning bpid BP-1607339391-127.0.1.1-1709696352182 on volume /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2024-03-05 22:39:13,796 [VolumeScannerThread(/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(385)) - Now scanning bpid BP-1607339391-127.0.1.1-1709696352182 on volume /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2024-03-05 22:39:13,796 [VolumeScannerThread(/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(385)) - Now scanning bpid BP-1607339391-127.0.1.1-1709696352182 on volume /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4
2024-03-05 22:39:13,796 [VolumeScannerThread(/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(385)) - Now scanning bpid BP-1607339391-127.0.1.1-1709696352182 on volume /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3
2024-03-05 22:39:13,797 [VolumeScannerThread(/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO  datanode.VolumeScanner (VolumeScanner.java:getNextBlockToScan(505)) - VolumeScanner(/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, DS-dbb30921-c74f-4995-8ef2-b17475f34882): finished scanning block pool BP-1607339391-127.0.1.1-1709696352182
2024-03-05 22:39:13,797 [VolumeScannerThread(/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:getNextBlockToScan(505)) - VolumeScanner(/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-a0953621-3236-40ef-8e69-cbc77fe0dd76): finished scanning block pool BP-1607339391-127.0.1.1-1709696352182
2024-03-05 22:39:13,797 [VolumeScannerThread(/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:getNextBlockToScan(505)) - VolumeScanner(/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-fb698f9f-eda6-4ec4-8154-78ab02d10859): finished scanning block pool BP-1607339391-127.0.1.1-1709696352182
2024-03-05 22:39:13,797 [VolumeScannerThread(/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3)] INFO  datanode.VolumeScanner (VolumeScanner.java:getNextBlockToScan(505)) - VolumeScanner(/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3, DS-19547c29-546b-4b55-86e8-03fd99bb066e): finished scanning block pool BP-1607339391-127.0.1.1-1709696352182
2024-03-05 22:39:13,797 [Thread-75] WARN  datanode.DirectoryScanner (DirectoryScanner.java:<init>(302)) - dfs.datanode.directoryscan.throttle.limit.ms.per.sec set to value above 1000 ms/sec. Assuming default value of -1
2024-03-05 22:39:13,797 [Thread-104] WARN  datanode.DirectoryScanner (DirectoryScanner.java:<init>(302)) - dfs.datanode.directoryscan.throttle.limit.ms.per.sec set to value above 1000 ms/sec. Assuming default value of -1
2024-03-05 22:39:13,798 [Thread-75] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(367)) - Periodic Directory Tree Verification scan starting in 1859950ms with interval of 21600000ms and throttle limit of -1ms/s
2024-03-05 22:39:13,798 [Thread-104] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(367)) - Periodic Directory Tree Verification scan starting in 15251206ms with interval of 21600000ms and throttle limit of -1ms/s
2024-03-05 22:39:13,801 [BP-1607339391-127.0.1.1-1709696352182 heartbeating to localhost/127.0.0.1:36175] INFO  datanode.DataNode (BPServiceActor.java:register(828)) - Block pool BP-1607339391-127.0.1.1-1709696352182 (Datanode Uuid e0db7eb1-f861-4de4-a0df-2a2a3acfcfcd) service to localhost/127.0.0.1:36175 beginning handshake with NN: localhost/127.0.0.1:36175.
2024-03-05 22:39:13,801 [BP-1607339391-127.0.1.1-1709696352182 heartbeating to localhost/127.0.0.1:36175] INFO  datanode.DataNode (BPServiceActor.java:register(828)) - Block pool BP-1607339391-127.0.1.1-1709696352182 (Datanode Uuid e4a85aef-13ce-4b47-8b3c-6054f58e20ea) service to localhost/127.0.0.1:36175 beginning handshake with NN: localhost/127.0.0.1:36175.
dn.shadowIpaddr is127.0.0.1
dn.shadowIpaddr is127.0.0.1
dn.shadowXferPort is45427
dn.shadowXferPort is39543
2024-03-05 22:39:13,809 [VolumeScannerThread(/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(402)) - VolumeScanner(/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, DS-dbb30921-c74f-4995-8ef2-b17475f34882): no suitable block pools found to scan.  Waiting 1814399987 ms.
2024-03-05 22:39:13,809 [VolumeScannerThread(/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(402)) - VolumeScanner(/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3, DS-19547c29-546b-4b55-86e8-03fd99bb066e): no suitable block pools found to scan.  Waiting 1814399987 ms.
2024-03-05 22:39:13,809 [VolumeScannerThread(/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(402)) - VolumeScanner(/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-fb698f9f-eda6-4ec4-8154-78ab02d10859): no suitable block pools found to scan.  Waiting 1814399987 ms.
2024-03-05 22:39:13,809 [VolumeScannerThread(/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(402)) - VolumeScanner(/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-a0953621-3236-40ef-8e69-cbc77fe0dd76): no suitable block pools found to scan.  Waiting 1814399987 ms.
2024-03-05 22:39:13,812 [IPC Server handler 4 on default port 36175] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1188)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:36261, datanodeUuid=e4a85aef-13ce-4b47-8b3c-6054f58e20ea, infoPort=38381, infoSecurePort=0, ipcPort=37983, storageInfo=lv=-57;cid=testClusterID;nsid=783508685;c=1709696352182) storage e4a85aef-13ce-4b47-8b3c-6054f58e20ea
2024-03-05 22:39:13,812 [IPC Server handler 4 on default port 36175] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:registerDatanode(1283)) - Failure Recovery: nodeDescr is127.0.0.1:45427
2024-03-05 22:39:13,812 [IPC Server handler 4 on default port 36175] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:registerDatanode(1284)) - Failure Recovery: nodeDescr shadow port is45427
2024-03-05 22:39:13,813 [IPC Server handler 4 on default port 36175] INFO  net.NetworkTopology (NetworkTopology.java:add(156)) - Adding a new node: /default-rack/127.0.0.1:36261
2024-03-05 22:39:13,813 [IPC Server handler 4 on default port 36175] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(200)) - Registered DN e4a85aef-13ce-4b47-8b3c-6054f58e20ea (127.0.0.1:36261).
2024-03-05 22:39:13,815 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2947)) - dnInfo.length != numDataNodes
2024-03-05 22:39:13,816 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2871)) - Waiting for cluster to become active
2024-03-05 22:39:13,816 [IPC Server handler 3 on default port 36175] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1188)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:39695, datanodeUuid=e0db7eb1-f861-4de4-a0df-2a2a3acfcfcd, infoPort=38133, infoSecurePort=0, ipcPort=35077, storageInfo=lv=-57;cid=testClusterID;nsid=783508685;c=1709696352182) storage e0db7eb1-f861-4de4-a0df-2a2a3acfcfcd
2024-03-05 22:39:13,817 [IPC Server handler 3 on default port 36175] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:registerDatanode(1283)) - Failure Recovery: nodeDescr is127.0.0.1:39543
2024-03-05 22:39:13,817 [IPC Server handler 3 on default port 36175] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:registerDatanode(1284)) - Failure Recovery: nodeDescr shadow port is39543
2024-03-05 22:39:13,817 [IPC Server handler 3 on default port 36175] INFO  net.NetworkTopology (NetworkTopology.java:add(156)) - Adding a new node: /default-rack/127.0.0.1:39695
2024-03-05 22:39:13,817 [IPC Server handler 3 on default port 36175] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(200)) - Registered DN e0db7eb1-f861-4de4-a0df-2a2a3acfcfcd (127.0.0.1:39695).
dn.shadowIpaddr is127.0.0.1
dn.shadowXferPort is45427
dn.shadowIpaddr is127.0.0.1
dn.shadowXferPort is39543
2024-03-05 22:39:13,818 [BP-1607339391-127.0.1.1-1709696352182 heartbeating to localhost/127.0.0.1:36175] INFO  datanode.DataNode (BPServiceActor.java:register(855)) - Block pool BP-1607339391-127.0.1.1-1709696352182 (Datanode Uuid e4a85aef-13ce-4b47-8b3c-6054f58e20ea) service to localhost/127.0.0.1:36175 successfully registered with NN: localhost/127.0.0.1:36175.
2024-03-05 22:39:13,818 [BP-1607339391-127.0.1.1-1709696352182 heartbeating to localhost/127.0.0.1:36175] INFO  datanode.DataNode (BPServiceActor.java:register(855)) - Block pool BP-1607339391-127.0.1.1-1709696352182 (Datanode Uuid e0db7eb1-f861-4de4-a0df-2a2a3acfcfcd) service to localhost/127.0.0.1:36175 successfully registered with NN: localhost/127.0.0.1:36175.
2024-03-05 22:39:13,818 [BP-1607339391-127.0.1.1-1709696352182 heartbeating to localhost/127.0.0.1:36175] INFO  datanode.DataNode (BPServiceActor.java:offerService(671)) - For namenode localhost/127.0.0.1:36175 using BLOCKREPORT_INTERVAL of 21600000msecs CACHEREPORT_INTERVAL of 10000msecs Initial delay: 0msecs; heartBeatInterval=3000
2024-03-05 22:39:13,818 [BP-1607339391-127.0.1.1-1709696352182 heartbeating to localhost/127.0.0.1:36175] INFO  datanode.DataNode (BPServiceActor.java:offerService(671)) - For namenode localhost/127.0.0.1:36175 using BLOCKREPORT_INTERVAL of 21600000msecs CACHEREPORT_INTERVAL of 10000msecs Initial delay: 0msecs; heartBeatInterval=3000
dn.shadowIpaddr is127.0.0.1
dn.shadowIpaddr is127.0.0.1
dn.shadowXferPort is39543
dn.shadowXferPort is45427
2024-03-05 22:39:13,827 [IPC Server handler 5 on default port 36175] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(1068)) - Adding new storage ID DS-fb698f9f-eda6-4ec4-8154-78ab02d10859 for DN 127.0.0.1:36261
2024-03-05 22:39:13,827 [IPC Server handler 5 on default port 36175] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(1068)) - Adding new storage ID DS-a0953621-3236-40ef-8e69-cbc77fe0dd76 for DN 127.0.0.1:36261
2024-03-05 22:39:13,827 [IPC Server handler 7 on default port 36175] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(1068)) - Adding new storage ID DS-19547c29-546b-4b55-86e8-03fd99bb066e for DN 127.0.0.1:39695
2024-03-05 22:39:13,827 [IPC Server handler 7 on default port 36175] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(1068)) - Adding new storage ID DS-dbb30921-c74f-4995-8ef2-b17475f34882 for DN 127.0.0.1:39695
2024-03-05 22:39:13,832 [BP-1607339391-127.0.1.1-1709696352182 heartbeating to localhost/127.0.0.1:36175] INFO  datanode.DataNode (BPServiceActor.java:offerService(733)) - After receiving heartbeat response, updating state of namenode localhost:36175 to active
2024-03-05 22:39:13,832 [BP-1607339391-127.0.1.1-1709696352182 heartbeating to localhost/127.0.0.1:36175] INFO  datanode.DataNode (BPServiceActor.java:offerService(733)) - After receiving heartbeat response, updating state of namenode localhost:36175 to active
dn.shadowIpaddr is127.0.0.1
dn.shadowXferPort is39543
dn.shadowIpaddr is127.0.0.1
dn.shadowXferPort is45427
2024-03-05 22:39:13,841 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2944)) - BLOCK* processReport 0xf0aad6d0cec1af7f with lease ID 0x88809da152a01dd6: Processing first storage report for DS-a0953621-3236-40ef-8e69-cbc77fe0dd76 from datanode DatanodeRegistration(127.0.0.1:36261, datanodeUuid=e4a85aef-13ce-4b47-8b3c-6054f58e20ea, infoPort=38381, infoSecurePort=0, ipcPort=37983, storageInfo=lv=-57;cid=testClusterID;nsid=783508685;c=1709696352182)
2024-03-05 22:39:13,842 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2976)) - BLOCK* processReport 0xf0aad6d0cec1af7f with lease ID 0x88809da152a01dd6: from storage DS-a0953621-3236-40ef-8e69-cbc77fe0dd76 node DatanodeRegistration(127.0.0.1:36261, datanodeUuid=e4a85aef-13ce-4b47-8b3c-6054f58e20ea, infoPort=38381, infoSecurePort=0, ipcPort=37983, storageInfo=lv=-57;cid=testClusterID;nsid=783508685;c=1709696352182), blocks: 0, hasStaleStorage: true, processing time: 1 msecs, invalidatedBlocks: 0
2024-03-05 22:39:13,842 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2944)) - BLOCK* processReport 0xaabf911c343297f5 with lease ID 0x88809da152a01dd7: Processing first storage report for DS-dbb30921-c74f-4995-8ef2-b17475f34882 from datanode DatanodeRegistration(127.0.0.1:39695, datanodeUuid=e0db7eb1-f861-4de4-a0df-2a2a3acfcfcd, infoPort=38133, infoSecurePort=0, ipcPort=35077, storageInfo=lv=-57;cid=testClusterID;nsid=783508685;c=1709696352182)
2024-03-05 22:39:13,842 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2976)) - BLOCK* processReport 0xaabf911c343297f5 with lease ID 0x88809da152a01dd7: from storage DS-dbb30921-c74f-4995-8ef2-b17475f34882 node DatanodeRegistration(127.0.0.1:39695, datanodeUuid=e0db7eb1-f861-4de4-a0df-2a2a3acfcfcd, infoPort=38133, infoSecurePort=0, ipcPort=35077, storageInfo=lv=-57;cid=testClusterID;nsid=783508685;c=1709696352182), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2024-03-05 22:39:13,842 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2944)) - BLOCK* processReport 0xf0aad6d0cec1af7f with lease ID 0x88809da152a01dd6: Processing first storage report for DS-fb698f9f-eda6-4ec4-8154-78ab02d10859 from datanode DatanodeRegistration(127.0.0.1:36261, datanodeUuid=e4a85aef-13ce-4b47-8b3c-6054f58e20ea, infoPort=38381, infoSecurePort=0, ipcPort=37983, storageInfo=lv=-57;cid=testClusterID;nsid=783508685;c=1709696352182)
2024-03-05 22:39:13,842 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2976)) - BLOCK* processReport 0xf0aad6d0cec1af7f with lease ID 0x88809da152a01dd6: from storage DS-fb698f9f-eda6-4ec4-8154-78ab02d10859 node DatanodeRegistration(127.0.0.1:36261, datanodeUuid=e4a85aef-13ce-4b47-8b3c-6054f58e20ea, infoPort=38381, infoSecurePort=0, ipcPort=37983, storageInfo=lv=-57;cid=testClusterID;nsid=783508685;c=1709696352182), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2024-03-05 22:39:13,842 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2944)) - BLOCK* processReport 0xaabf911c343297f5 with lease ID 0x88809da152a01dd7: Processing first storage report for DS-19547c29-546b-4b55-86e8-03fd99bb066e from datanode DatanodeRegistration(127.0.0.1:39695, datanodeUuid=e0db7eb1-f861-4de4-a0df-2a2a3acfcfcd, infoPort=38133, infoSecurePort=0, ipcPort=35077, storageInfo=lv=-57;cid=testClusterID;nsid=783508685;c=1709696352182)
2024-03-05 22:39:13,842 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2976)) - BLOCK* processReport 0xaabf911c343297f5 with lease ID 0x88809da152a01dd7: from storage DS-19547c29-546b-4b55-86e8-03fd99bb066e node DatanodeRegistration(127.0.0.1:39695, datanodeUuid=e0db7eb1-f861-4de4-a0df-2a2a3acfcfcd, infoPort=38133, infoSecurePort=0, ipcPort=35077, storageInfo=lv=-57;cid=testClusterID;nsid=783508685;c=1709696352182), blocks: 0, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
2024-03-05 22:39:13,850 [BP-1607339391-127.0.1.1-1709696352182 heartbeating to localhost/127.0.0.1:36175] INFO  datanode.DataNode (BPServiceActor.java:blockReport(457)) - Successfully sent block report 0xf0aad6d0cec1af7f with lease ID 0x88809da152a01dd6 to namenode: localhost/127.0.0.1:36175,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 2 msecs to generate and 15 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2024-03-05 22:39:13,850 [BP-1607339391-127.0.1.1-1709696352182 heartbeating to localhost/127.0.0.1:36175] INFO  datanode.DataNode (BPServiceActor.java:blockReport(457)) - Successfully sent block report 0xaabf911c343297f5 with lease ID 0x88809da152a01dd7 to namenode: localhost/127.0.0.1:36175,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 2 msecs to generate and 15 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2024-03-05 22:39:13,851 [Command processor] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(767)) - Got finalize command for block pool BP-1607339391-127.0.1.1-1709696352182
2024-03-05 22:39:13,851 [Command processor] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(767)) - Got finalize command for block pool BP-1607339391-127.0.1.1-1709696352182
dn.shadowIpaddr is127.0.0.1
dn.shadowXferPort is45427
dn.shadowIpaddr is127.0.0.1
dn.shadowXferPort is39543
Failure Recovery convert 788 45427
Failure Recovery convert 816 45427
Failure Recovery convert 788 39543
Failure Recovery convert 816 39543
2024-03-05 22:39:13,928 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2923)) - Cluster is active
dn.shadowIpaddr is127.0.0.1
dn.shadowXferPort is45427
dn.shadowIpaddr is127.0.0.1
dn.shadowXferPort is39543
Failure Recovery convert 788 45427
Failure Recovery convert 816 45427
Failure Recovery convert 788 39543
Failure Recovery convert 816 39543
2024-03-05 22:39:13,932 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2923)) - Cluster is active
2024-03-05 22:39:14,033 [Thread-145] INFO  hdfs.DataStreamer (DataStreamer.java:run(777)) - Before shadowErrorHandler, the nodes are: null
2024-03-05 22:39:14,033 [Thread-144] INFO  hdfs.DataStreamer (ShadowDataStreamer.java:run(878)) - Before shadowErrorHandler, the nodes are: null
2024-03-05 22:39:14,033 [Thread-144] INFO  hdfs.DataStreamer (ShadowDataStreamer.java:run(882)) - Failure Recovery is paused.
2024-03-05 22:39:14,046 [IPC Server handler 2 on default port 36175] INFO  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseRandom(925)) - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1}
2024-03-05 22:39:14,046 [IPC Server handler 2 on default port 36175] INFO  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:getPipeline(1181)) - Failure Recovery: the shadow port is45427
2024-03-05 22:39:14,046 [IPC Server handler 2 on default port 36175] INFO  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:getPipeline(1182)) - Failure Recovery: the shadow port is36261
2024-03-05 22:39:14,046 [IPC Server handler 2 on default port 36175] INFO  blockmanagement.BlockManager (BlockManager.java:chooseTarget4NewBlock(2469)) - Failure Recovery: the shadow port is45427
2024-03-05 22:39:14,046 [IPC Server handler 2 on default port 36175] INFO  blockmanagement.BlockManager (BlockManager.java:chooseTarget4NewBlock(2470)) - Failure Recovery: the shadow port is36261
2024-03-05 22:39:14,046 [IPC Server handler 2 on default port 36175] INFO  namenode.FSNamesystem (FSNamesystem.java:getAdditionalBlock(3079)) - Failure Recovery: the shadow port is45427
2024-03-05 22:39:14,046 [IPC Server handler 2 on default port 36175] INFO  namenode.FSNamesystem (FSNamesystem.java:getAdditionalBlock(3080)) - Failure Recovery: the shadow port is36261
2024-03-05 22:39:14,048 [IPC Server handler 2 on default port 36175] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(801)) - BLOCK* allocate blk_1073741825_1001, replicas=127.0.0.1:36261, 127.0.0.1:39695 for /user/zhenyu/noheartbeat.dat
Failure Recovery: newLocatedBLock45427
Failure Recovery: newLocatedBlock36261
Failure Recovery: convert DatanodeInfoWithStorage the shadow port is45427
Failure Recovery: convert DatanodeInfoWithStorage the shadow port is36261
Failure Recovery: the shadow port is 11345427
Failure Recovery: the shadow port is 11336261
Failure Recovery: 108 the shadow port is45427
Failure Recovery: 108 the shadow port is36261
2024-03-05 22:39:14,049 [IPC Server handler 2 on default port 36175] INFO  namenode.NameNode (NameNodeRpcServer.java:addBlock(938)) - Failure Recovery add block namenoderpc 45427
dn.shadowIpaddr is127.0.0.1
dn.shadowXferPort is45427
dn.shadowIpaddr is127.0.0.1
dn.shadowXferPort is39543
Failure Recovery PBHelperClient45427
Failure Recovery convert 788 45427
Failure Recovery convert 816 45427
Failure Recovery convert 788 39543
Failure Recovery convert 816 39543
Failure Recovery LocatedBlock691 45427
Failure Recovery: convert DatanodeInfoWithStorage the shadow port is45427
Failure Recovery: convert DatanodeInfoWithStorage the shadow port is36261
Failure Recovery: the shadow port is 11345427
Failure Recovery: the shadow port is 11336261
Failure Recovery: 108 the shadow port is45427
Failure Recovery: 108 the shadow port is36261
Failure Recovery ClientNamenodeProtocol PBHelper shadowport is45427
2024-03-05 22:39:14,060 [Thread-145] INFO  hdfs.DataStreamer (DataStreamer.java:nextBlockOutputStream(2252)) - Failure Recovery 224545427
2024-03-05 22:39:14,060 [Thread-145] INFO  hdfs.DataStreamer (DataStreamer.java:createSocketForPipeline(244)) - Connecting to datanode 127.0.0.1:36261
2024-03-05 22:39:14,060 [Thread-145] INFO  hdfs.DataStreamer (DataStreamer.java:createSocketForPipeline(256)) - Send buf size 1313280
2024-03-05 22:39:14,060 [Thread-145] INFO  hdfs.DataStreamer (DataStreamer.java:createSocketForPipeline(257)) - DataStreamer local address is 127.0.0.1:50372
2024-03-05 22:39:14,060 [Thread-145] INFO  hdfs.DataStreamer (DataStreamer.java:createSocketForPipeline(258)) - DataStreamer remote address is 127.0.0.1:36261
2024-03-05 22:39:14,060 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@62ef27a8] INFO  datanode.DataNode (DataXceiverServer.java:run(278)) - Accepted a new connection: 0 false
2024-03-05 22:39:14,067 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@62ef27a8] INFO  datanode.DataNode (DataXceiver.java:<init>(178)) - Remote address is: /127.0.0.1:50372, false
2024-03-05 22:39:14,067 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@62ef27a8] INFO  datanode.DataNode (DataXceiver.java:<init>(183)) - Local address is: /127.0.0.1:36261, false
2024-03-05 22:39:14,068 [org.apache.hadoop.hdfs.server.datanode.DataXceiver@7164374] INFO  datanode.DataNode (DataXceiver.java:run(259)) - Failure Recovery isShadow is 257false
2024-03-05 22:39:14,068 [org.apache.hadoop.hdfs.server.datanode.DataXceiver@7164374] INFO  datanode.DataNode (DataXceiver.java:run(267)) - Failure Recovery isShadow is 265false
2024-03-05 22:39:14,069 [org.apache.hadoop.hdfs.server.datanode.DataXceiver@7164374] INFO  datanode.DataNode (DataXceiver.java:run(291)) - Failure Recovery isShadow is 289false
2024-03-05 22:39:14,069 [DataXceiver for client /127.0.0.1:50372 [Waiting for operation #1]] INFO  datanode.DataNode (DataXceiver.java:run(307)) - Failure Recovery isShadow is beforereadOpfalse
2024-03-05 22:39:14,072 [Thread-145] INFO  hdfs.DataStreamer (DataStreamer.java:createBlockOutputStream(2336)) - Failure Recovery 2330 badnode index is-1
dn.shadowIpaddr is127.0.0.1
dn.shadowXferPort is39543
2024-03-05 22:39:14,085 [Thread-145] INFO  datatransfer.DataTransferProtocol (Sender.java:writeBlock(187)) - Failure Recovery Sender before
2024-03-05 22:39:14,095 [Thread-145] INFO  datatransfer.DataTransferProtocol (Sender.java:send(82)) - Failure Recovery send 7 
2024-03-05 22:39:14,095 [Thread-145] INFO  datatransfer.DataTransferProtocol (Sender.java:send(84)) - Failure Recovery send 8
2024-03-05 22:39:14,095 [Thread-145] INFO  datatransfer.DataTransferProtocol (Sender.java:send(86)) - Failure Recovery send 9
2024-03-05 22:39:14,095 [Thread-145] INFO  datatransfer.DataTransferProtocol (Sender.java:send(88)) - Failure Recovery send 10
2024-03-05 22:39:14,095 [Thread-145] INFO  datatransfer.DataTransferProtocol (Sender.java:writeBlock(191)) - Failure Recovery Sender after
2024-03-05 22:39:14,096 [DataXceiver for client /127.0.0.1:50372 [Waiting for operation #1]] INFO  datanode.DataNode (DataXceiver.java:run(309)) - Failure Recovery isShadow is aftereadOpfalse
2024-03-05 22:39:14,096 [DataXceiver for client /127.0.0.1:50372 [Waiting for operation #1]] INFO  datanode.DataNode (DataXceiver.java:run(324)) - Failure Recovery isShadow is 322false
2024-03-05 22:39:14,096 [DataXceiver for client /127.0.0.1:50372 [Waiting for operation #1]] INFO  datanode.DataNode (DataXceiver.java:run(331)) - Failure Recovery isShadow is 329false
2024-03-05 22:39:14,096 [DataXceiver for client /127.0.0.1:50372 [Waiting for operation #1]] INFO  datanode.DataNode (DataXceiver.java:run(339)) - Failure Recovery isShadow is 333false
Firstbyte is 157
Failure Recovery convert 788 39543
Failure Recovery convert 816 39543
Failure Recovery convert 788 0
Failure Recovery convert 816 0
2024-03-05 22:39:14,097 [DataXceiver for client DFSClient_NONMAPREDUCE_-1963227005_1 at /127.0.0.1:50372 [Receiving block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(805)) - Receiving BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001 src: /127.0.0.1:50372 dest: /127.0.0.1:36261
2024-03-05 22:39:14,110 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@37bd68c3] INFO  datanode.DataNode (DataXceiverServer.java:run(278)) - Accepted a new connection: 0 false
dn.shadowIpaddr is
dn.shadowXferPort is0
2024-03-05 22:39:14,110 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@37bd68c3] INFO  datanode.DataNode (DataXceiver.java:<init>(178)) - Remote address is: /127.0.0.1:42402, false
2024-03-05 22:39:14,110 [DataXceiver for client DFSClient_NONMAPREDUCE_-1963227005_1 at /127.0.0.1:50372 [Receiving block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001]] INFO  datatransfer.DataTransferProtocol (Sender.java:writeBlock(187)) - Failure Recovery Sender before
2024-03-05 22:39:14,110 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@37bd68c3] INFO  datanode.DataNode (DataXceiver.java:<init>(183)) - Local address is: /127.0.0.1:39695, false
2024-03-05 22:39:14,110 [DataXceiver for client DFSClient_NONMAPREDUCE_-1963227005_1 at /127.0.0.1:50372 [Receiving block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001]] INFO  datatransfer.DataTransferProtocol (Sender.java:send(82)) - Failure Recovery send 7 
2024-03-05 22:39:14,111 [DataXceiver for client DFSClient_NONMAPREDUCE_-1963227005_1 at /127.0.0.1:50372 [Receiving block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001]] INFO  datatransfer.DataTransferProtocol (Sender.java:send(84)) - Failure Recovery send 8
2024-03-05 22:39:14,111 [DataXceiver for client DFSClient_NONMAPREDUCE_-1963227005_1 at /127.0.0.1:50372 [Receiving block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001]] INFO  datatransfer.DataTransferProtocol (Sender.java:send(86)) - Failure Recovery send 9
2024-03-05 22:39:14,111 [DataXceiver for client DFSClient_NONMAPREDUCE_-1963227005_1 at /127.0.0.1:50372 [Receiving block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001]] INFO  datatransfer.DataTransferProtocol (Sender.java:send(88)) - Failure Recovery send 10
2024-03-05 22:39:14,111 [DataXceiver for client DFSClient_NONMAPREDUCE_-1963227005_1 at /127.0.0.1:50372 [Receiving block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001]] INFO  datatransfer.DataTransferProtocol (Sender.java:writeBlock(191)) - Failure Recovery Sender after
2024-03-05 22:39:14,111 [org.apache.hadoop.hdfs.server.datanode.DataXceiver@716f7bf9] INFO  datanode.DataNode (DataXceiver.java:run(259)) - Failure Recovery isShadow is 257false
2024-03-05 22:39:14,111 [org.apache.hadoop.hdfs.server.datanode.DataXceiver@716f7bf9] INFO  datanode.DataNode (DataXceiver.java:run(267)) - Failure Recovery isShadow is 265false
2024-03-05 22:39:14,111 [org.apache.hadoop.hdfs.server.datanode.DataXceiver@716f7bf9] INFO  datanode.DataNode (DataXceiver.java:run(291)) - Failure Recovery isShadow is 289false
2024-03-05 22:39:14,111 [DataXceiver for client /127.0.0.1:42402 [Waiting for operation #1]] INFO  datanode.DataNode (DataXceiver.java:run(307)) - Failure Recovery isShadow is beforereadOpfalse
2024-03-05 22:39:14,111 [DataXceiver for client /127.0.0.1:42402 [Waiting for operation #1]] INFO  datanode.DataNode (DataXceiver.java:run(309)) - Failure Recovery isShadow is aftereadOpfalse
2024-03-05 22:39:14,111 [DataXceiver for client /127.0.0.1:42402 [Waiting for operation #1]] INFO  datanode.DataNode (DataXceiver.java:run(324)) - Failure Recovery isShadow is 322false
2024-03-05 22:39:14,111 [DataXceiver for client /127.0.0.1:42402 [Waiting for operation #1]] INFO  datanode.DataNode (DataXceiver.java:run(331)) - Failure Recovery isShadow is 329false
2024-03-05 22:39:14,111 [DataXceiver for client /127.0.0.1:42402 [Waiting for operation #1]] INFO  datanode.DataNode (DataXceiver.java:run(339)) - Failure Recovery isShadow is 333false
Firstbyte is 229
Failure Recovery convert 788 0
Failure Recovery convert 816 0
2024-03-05 22:39:14,112 [DataXceiver for client DFSClient_NONMAPREDUCE_-1963227005_1 at /127.0.0.1:42402 [Receiving block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(805)) - Receiving BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001 src: /127.0.0.1:42402 dest: /127.0.0.1:39695
2024-03-05 22:39:14,114 [DataXceiver for client DFSClient_NONMAPREDUCE_-1963227005_1 at /127.0.0.1:42402 [Receiving block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(960)) - WriteBlock mirrorInStatus is SUCCESS, firstBadLink is 
2024-03-05 22:39:14,114 [DataXceiver for client DFSClient_NONMAPREDUCE_-1963227005_1 at /127.0.0.1:42402 [Receiving block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(962)) - DataStreamer: flush replyOut successfully
Firstbyte is 4
2024-03-05 22:39:14,114 [DataXceiver for client DFSClient_NONMAPREDUCE_-1963227005_1 at /127.0.0.1:50372 [Receiving block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(960)) - WriteBlock mirrorInStatus is SUCCESS, firstBadLink is 
Firstbyte is 4
2024-03-05 22:39:14,115 [DataXceiver for client DFSClient_NONMAPREDUCE_-1963227005_1 at /127.0.0.1:50372 [Receiving block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(962)) - DataStreamer: flush replyOut successfully
2024-03-05 22:39:14,116 [DataStreamer for file /user/zhenyu/noheartbeat.dat block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001] INFO  hdfs.DataStreamer (DataStreamer.java:run(777)) - Before shadowErrorHandler, the nodes are: [DatanodeInfoWithStorage[127.0.0.1:36261,DS-a0953621-3236-40ef-8e69-cbc77fe0dd76,DISK], DatanodeInfoWithStorage[127.0.0.1:39695,DS-dbb30921-c74f-4995-8ef2-b17475f34882,DISK]]
Firstbyte is 9
Firstbyte is 15
[Failure Recovery] The length of ack is 2
2024-03-05 22:39:14,123 [IPC Server handler 3 on default port 36175] INFO  hdfs.StateChange (FSNamesystem.java:fsync(3674)) - BLOCK* fsync: /user/zhenyu/noheartbeat.dat for DFSClient_NONMAPREDUCE_-1963227005_1
2024-03-05 22:39:14,124 [DataStreamer for file /user/zhenyu/noheartbeat.dat block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001] INFO  hdfs.DataStreamer (DataStreamer.java:run(777)) - Before shadowErrorHandler, the nodes are: [DatanodeInfoWithStorage[127.0.0.1:36261,DS-a0953621-3236-40ef-8e69-cbc77fe0dd76,DISK], DatanodeInfoWithStorage[127.0.0.1:39695,DS-dbb30921-c74f-4995-8ef2-b17475f34882,DISK]]
Firstbyte is 9
2024-03-05 22:39:14,125 [PacketResponder: BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:39695]] INFO  datanode.DataNode (BlockReceiver.java:run(1987)) - PacketResponder: BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:39695]
java.io.IOException: Remove bad datanode
	at org.apache.hadoop.hdfs.TestClientProtocolForPipelineRecovery$4.markBadNode(TestClientProtocolForPipelineRecovery.java:395)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.run(BlockReceiver.java:1909)
	at java.lang.Thread.run(Thread.java:750)
Firstbyte is 12
[Failure Recovery] The length of ack is 2
2024-03-05 22:39:14,126 [ResponseProcessor for block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001] WARN  hdfs.DataStreamer (DataStreamer.java:run(1375)) - Exception for BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001
java.io.IOException: Bad response ERROR for BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001 from datanode DatanodeInfoWithStorage[127.0.0.1:39695,DS-dbb30921-c74f-4995-8ef2-b17475f34882,DISK]
	at org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1295)
2024-03-05 22:39:14,127 [DataStreamer for file /user/zhenyu/noheartbeat.dat block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001] INFO  hdfs.DataStreamer (DataStreamer.java:run(777)) - Before shadowErrorHandler, the nodes are: [DatanodeInfoWithStorage[127.0.0.1:36261,DS-a0953621-3236-40ef-8e69-cbc77fe0dd76,DISK], DatanodeInfoWithStorage[127.0.0.1:39695,DS-dbb30921-c74f-4995-8ef2-b17475f34882,DISK]]
2024-03-05 22:39:14,127 [DataXceiver for client DFSClient_NONMAPREDUCE_-1963227005_1 at /127.0.0.1:50372 [Receiving block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001]] INFO  datanode.DataNode (BlockReceiver.java:receiveBlock(1538)) - Exception for BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001
java.io.IOException: Premature EOF from inputStream
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:216)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:221)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:144)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:119)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:705)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:1499)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:968)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:219)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:110)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:340)
	at java.lang.Thread.run(Thread.java:750)
2024-03-05 22:39:14,127 [DataStreamer for file /user/zhenyu/noheartbeat.dat block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001] INFO  hdfs.DataStreamer (DataStreamer.java:handleBadDatanode(2102)) - Failure Recovery: prepare For Processing 0
2024-03-05 22:39:14,127 [PacketResponder: BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:39695]] WARN  datanode.DataNode (BlockReceiver.java:run(2025)) - IOException in PacketResponder.run(): 
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
	at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47)
	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)
	at sun.nio.ch.IOUtil.write(IOUtil.java:65)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:470)
	at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:62)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:141)
	at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:158)
	at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:116)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
	at java.io.DataOutputStream.flush(DataOutputStream.java:123)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.sendAckUpstreamUnprotected(BlockReceiver.java:2177)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.sendAckUpstream(BlockReceiver.java:2108)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.run(BlockReceiver.java:2016)
	at java.lang.Thread.run(Thread.java:750)
2024-03-05 22:39:14,127 [PacketResponder: BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:39695]] INFO  datanode.DataNode (BlockReceiver.java:run(2028)) - PacketResponder: BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:39695]
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
	at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47)
	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)
	at sun.nio.ch.IOUtil.write(IOUtil.java:65)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:470)
	at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:62)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:141)
	at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:158)
	at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:116)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
	at java.io.DataOutputStream.flush(DataOutputStream.java:123)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.sendAckUpstreamUnprotected(BlockReceiver.java:2177)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.sendAckUpstream(BlockReceiver.java:2108)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.run(BlockReceiver.java:2016)
	at java.lang.Thread.run(Thread.java:750)
2024-03-05 22:39:14,128 [PacketResponder: BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:39695]] INFO  datanode.DataNode (BlockReceiver.java:run(2045)) - PacketResponder: BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[127.0.0.1:39695] terminating
2024-03-05 22:39:14,128 [DataXceiver for client DFSClient_NONMAPREDUCE_-1963227005_1 at /127.0.0.1:50372 [Receiving block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(1000)) - opWriteBlock BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001 received exception java.io.IOException: Premature EOF from inputStream
2024-03-05 22:39:14,128 [DataXceiver for client DFSClient_NONMAPREDUCE_-1963227005_1 at /127.0.0.1:42402 [Receiving block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001]] INFO  datanode.DataNode (BlockReceiver.java:receiveBlock(1538)) - Exception for BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001
java.io.IOException: Premature EOF from inputStream
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:216)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:221)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:144)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:119)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:705)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:1499)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:968)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:219)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:110)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:340)
	at java.lang.Thread.run(Thread.java:750)
2024-03-05 22:39:14,128 [DataXceiver for client DFSClient_NONMAPREDUCE_-1963227005_1 at /127.0.0.1:50372 [Receiving block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001]] ERROR datanode.DataNode (DataXceiver.java:run(373)) - 127.0.0.1:36261:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:50372 dst: /127.0.0.1:36261
java.io.IOException: Premature EOF from inputStream
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:216)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:221)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:144)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:119)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:705)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:1499)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:968)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:219)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:110)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:340)
	at java.lang.Thread.run(Thread.java:750)
2024-03-05 22:39:14,128 [PacketResponder: BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(2003)) - PacketResponder: BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001, type=LAST_IN_PIPELINE: Thread is interrupted.
2024-03-05 22:39:14,129 [PacketResponder: BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(2045)) - PacketResponder: BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001, type=LAST_IN_PIPELINE terminating
2024-03-05 22:39:14,129 [DataXceiver for client DFSClient_NONMAPREDUCE_-1963227005_1 at /127.0.0.1:42402 [Receiving block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(1000)) - opWriteBlock BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001 received exception java.io.IOException: Premature EOF from inputStream
2024-03-05 22:39:14,129 [DataXceiver for client DFSClient_NONMAPREDUCE_-1963227005_1 at /127.0.0.1:42402 [Receiving block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001]] ERROR datanode.DataNode (DataXceiver.java:run(373)) - 127.0.0.1:39695:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:42402 dst: /127.0.0.1:39695
java.io.IOException: Premature EOF from inputStream
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:216)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:221)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:144)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:119)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:705)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:1499)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:968)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:219)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:110)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:340)
	at java.lang.Thread.run(Thread.java:750)
2024-03-05 22:39:16,127 [DataStreamer for file /user/zhenyu/noheartbeat.dat block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001] INFO  hdfs.DataStreamer (ShadowDataStreamer.java:prepareForProcessing(591)) - ShadowDataStreamer: prepare For Processing
2024-03-05 22:39:16,128 [DataStreamer for file /user/zhenyu/noheartbeat.dat block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001] INFO  hdfs.DataStreamer (ShadowDataStreamer.java:copyFromDataStreamer(561)) - Failure Recovery: prepare For Processing
2024-03-05 22:39:16,128 [DataStreamer for file /user/zhenyu/noheartbeat.dat block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001] INFO  hdfs.DataStreamer (ShadowDataStreamer.java:copyFromDataStreamer(564)) - ShadowDataStreamer: ErrorState: 1
2024-03-05 22:39:16,128 [Thread-144] INFO  hdfs.DataStreamer (ShadowDataStreamer.java:run(888)) - Failure Recovery is relived.
2024-03-05 22:39:16,128 [DataStreamer for file /user/zhenyu/noheartbeat.dat block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001] WARN  hdfs.DataStreamer (DataStreamer.java:handleBadDatanode(2122)) - Error Recovery for BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001 in pipeline [DatanodeInfoWithStorage[127.0.0.1:36261,DS-a0953621-3236-40ef-8e69-cbc77fe0dd76,DISK], DatanodeInfoWithStorage[127.0.0.1:39695,DS-dbb30921-c74f-4995-8ef2-b17475f34882,DISK]]: datanode 1(DatanodeInfoWithStorage[127.0.0.1:39695,DS-dbb30921-c74f-4995-8ef2-b17475f34882,DISK]) is bad.
2024-03-05 22:39:16,128 [Thread-144] INFO  hdfs.DataStreamer (ShadowDataStreamer.java:processDatanodeOrExternalError(1660)) - SDS: 1566
2024-03-05 22:39:16,128 [Thread-144] INFO  hdfs.DataStreamer (ShadowDataStreamer.java:processDatanodeOrExternalError(1671)) - SDS: 1577
2024-03-05 22:39:16,128 [Thread-144] INFO  hdfs.DataStreamer (ShadowDataStreamer.java:processDatanodeOrExternalError(1695)) - SDS: 1601
2024-03-05 22:39:16,128 [Thread-144] INFO  hdfs.DataStreamer (ShadowDataStreamer.java:setupPipelineInternal(2108)) - ShadowDataStreamer: setupPipelineInternal
2024-03-05 22:39:16,128 [Thread-144] INFO  hdfs.DataStreamer (ShadowDataStreamer.java:handleBadDatanode(2243)) - ShadowDataStreamer: handleBadDatanode badNodeIndex: 1
2024-03-05 22:39:16,128 [Thread-144] INFO  hdfs.DataStreamer (ShadowDataStreamer.java:handleBadDatanode(2246)) - ShadowDataStreamer: handleBadDatanode badNodeIndex: 1
2024-03-05 22:39:16,129 [Thread-144] WARN  hdfs.DataStreamer (ShadowDataStreamer.java:handleBadDatanode(2259)) - Error Recovery for BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001 in pipeline [DatanodeInfoWithStorage[127.0.0.1:36261,DS-a0953621-3236-40ef-8e69-cbc77fe0dd76,DISK], DatanodeInfoWithStorage[127.0.0.1:39695,DS-dbb30921-c74f-4995-8ef2-b17475f34882,DISK]]: datanode 1(DatanodeInfoWithStorage[127.0.0.1:39695,DS-dbb30921-c74f-4995-8ef2-b17475f34882,DISK]) is bad.
2024-03-05 22:39:16,129 [Thread-144] INFO  hdfs.DataStreamer (ShadowDataStreamer.java:setupPipelineInternal(2122)) - ShadowDataStreamer: before newGS, accessToken, 2133
2024-03-05 22:39:16,136 [IPC Server handler 7 on default port 36175] INFO  namenode.FSNamesystem (FSNamesystem.java:bumpBlockGenerationStamp(6018)) - bumpBlockGenerationStamp(BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1002, client=DFSClient_NONMAPREDUCE_-1963227005_1) success
2024-03-05 22:39:16,136 [IPC Server handler 5 on default port 36175] INFO  namenode.FSNamesystem (FSNamesystem.java:bumpBlockGenerationStamp(6018)) - bumpBlockGenerationStamp(BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1003, client=DFSClient_NONMAPREDUCE_-1963227005_1) success
[Failure Recovery]: after update the nodes length is1
2024-03-05 22:39:16,142 [Thread-144] INFO  hdfs.DataStreamer (ShadowDataStreamer.java:setupPipelineInternal(2127)) - ShadowDataStreamer: after newGS, accessToken, 2138
2024-03-05 22:39:16,142 [DataStreamer for file /user/zhenyu/noheartbeat.dat block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001] INFO  hdfs.DataStreamer (DataStreamer.java:createSocketForPipeline(244)) - Connecting to datanode 127.0.0.1:36261
2024-03-05 22:39:16,142 [Thread-144] INFO  hdfs.DataStreamer (ShadowDataStreamer.java:createBlockOutputStream(2435)) - ShadowDataStreamer: createBlockOutputStream
2024-03-05 22:39:16,142 [Thread-144] INFO  hdfs.DataStreamer (ShadowDataStreamer.java:createSocketForPipeline(248)) - ShadowDataStreamer Connecting to datanode 127.0.0.1:45427
2024-03-05 22:39:16,143 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@62ef27a8] INFO  datanode.DataNode (DataXceiverServer.java:run(278)) - Accepted a new connection: 0 false
2024-03-05 22:39:16,143 [DataStreamer for file /user/zhenyu/noheartbeat.dat block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001] INFO  hdfs.DataStreamer (DataStreamer.java:createSocketForPipeline(256)) - Send buf size 1313280
2024-03-05 22:39:16,143 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@62ef27a8] INFO  datanode.DataNode (DataXceiver.java:<init>(178)) - Remote address is: /127.0.0.1:50378, false
2024-03-05 22:39:16,143 [DataStreamer for file /user/zhenyu/noheartbeat.dat block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001] INFO  hdfs.DataStreamer (DataStreamer.java:createSocketForPipeline(257)) - DataStreamer local address is 127.0.0.1:50378
2024-03-05 22:39:16,143 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@62ef27a8] INFO  datanode.DataNode (DataXceiver.java:<init>(183)) - Local address is: /127.0.0.1:36261, false
2024-03-05 22:39:16,143 [DataStreamer for file /user/zhenyu/noheartbeat.dat block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001] INFO  hdfs.DataStreamer (DataStreamer.java:createSocketForPipeline(258)) - DataStreamer remote address is 127.0.0.1:36261
2024-03-05 22:39:16,143 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@6436a7db] INFO  datanode.DataNode (DataXceiverServer.java:run(278)) - Accepted a new connection: 0 true
2024-03-05 22:39:16,143 [DataStreamer for file /user/zhenyu/noheartbeat.dat block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001] INFO  hdfs.DataStreamer (DataStreamer.java:createBlockOutputStream(2336)) - Failure Recovery 2330 badnode index is-1
2024-03-05 22:39:16,143 [Thread-144] INFO  hdfs.DataStreamer (ShadowDataStreamer.java:createSocketForPipeline(260)) - Send buf size 1313280
2024-03-05 22:39:16,143 [org.apache.hadoop.hdfs.server.datanode.DataXceiver@9d1ade6] INFO  datanode.DataNode (DataXceiver.java:run(259)) - Failure Recovery isShadow is 257false
2024-03-05 22:39:16,143 [DataStreamer for file /user/zhenyu/noheartbeat.dat block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001] INFO  hdfs.DataStreamer (DataStreamer.java:createBlockOutputStream(2339)) - Failure Recovery 233045427
2024-03-05 22:39:16,143 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@6436a7db] INFO  datanode.DataNode (DataXceiver.java:<init>(178)) - Remote address is: /127.0.0.1:40976, true
2024-03-05 22:39:16,143 [org.apache.hadoop.hdfs.server.datanode.DataXceiver@9d1ade6] INFO  datanode.DataNode (DataXceiver.java:run(267)) - Failure Recovery isShadow is 265false
2024-03-05 22:39:16,144 [DataStreamer for file /user/zhenyu/noheartbeat.dat block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001] INFO  datatransfer.DataTransferProtocol (Sender.java:writeBlock(187)) - Failure Recovery Sender before
2024-03-05 22:39:16,143 [Thread-144] INFO  hdfs.DataStreamer (ShadowDataStreamer.java:createSocketForPipeline(261)) - ShadowDataStreamer local address is 127.0.0.1:40976
2024-03-05 22:39:16,144 [DataStreamer for file /user/zhenyu/noheartbeat.dat block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001] INFO  datatransfer.DataTransferProtocol (Sender.java:send(82)) - Failure Recovery send 7 
2024-03-05 22:39:16,144 [org.apache.hadoop.hdfs.server.datanode.DataXceiver@9d1ade6] INFO  datanode.DataNode (DataXceiver.java:run(291)) - Failure Recovery isShadow is 289false
2024-03-05 22:39:16,143 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@6436a7db] INFO  datanode.DataNode (DataXceiver.java:<init>(183)) - Local address is: /127.0.0.1:45427, true
2024-03-05 22:39:16,144 [DataXceiver for client /127.0.0.1:50378 [Waiting for operation #1]] INFO  datanode.DataNode (DataXceiver.java:run(307)) - Failure Recovery isShadow is beforereadOpfalse
2024-03-05 22:39:16,144 [DataStreamer for file /user/zhenyu/noheartbeat.dat block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001] INFO  datatransfer.DataTransferProtocol (Sender.java:send(84)) - Failure Recovery send 8
2024-03-05 22:39:16,144 [Thread-144] INFO  hdfs.DataStreamer (ShadowDataStreamer.java:createSocketForPipeline(262)) - ShadowDataStreamer remote address is 127.0.0.1:45427
2024-03-05 22:39:16,144 [DataStreamer for file /user/zhenyu/noheartbeat.dat block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001] INFO  datatransfer.DataTransferProtocol (Sender.java:send(86)) - Failure Recovery send 9
2024-03-05 22:39:16,145 [org.apache.hadoop.hdfs.server.datanode.DataXceiver@24b42c99] INFO  datanode.DataNode (DataXceiver.java:run(259)) - Failure Recovery isShadow is 257true
2024-03-05 22:39:16,145 [DataStreamer for file /user/zhenyu/noheartbeat.dat block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001] INFO  datatransfer.DataTransferProtocol (Sender.java:send(88)) - Failure Recovery send 10
2024-03-05 22:39:16,145 [DataStreamer for file /user/zhenyu/noheartbeat.dat block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001] INFO  datatransfer.DataTransferProtocol (Sender.java:writeBlock(191)) - Failure Recovery Sender after
2024-03-05 22:39:16,145 [org.apache.hadoop.hdfs.server.datanode.DataXceiver@24b42c99] INFO  datanode.DataNode (DataXceiver.java:run(267)) - Failure Recovery isShadow is 265true
2024-03-05 22:39:16,145 [DataXceiver for client /127.0.0.1:50378 [Waiting for operation #1]] INFO  datanode.DataNode (DataXceiver.java:run(309)) - Failure Recovery isShadow is aftereadOpfalse
2024-03-05 22:39:16,145 [DataXceiver for client /127.0.0.1:50378 [Waiting for operation #1]] INFO  datanode.DataNode (DataXceiver.java:run(324)) - Failure Recovery isShadow is 322false
2024-03-05 22:39:16,145 [DataXceiver for client /127.0.0.1:50378 [Waiting for operation #1]] INFO  datanode.DataNode (DataXceiver.java:run(331)) - Failure Recovery isShadow is 329false
2024-03-05 22:39:16,145 [org.apache.hadoop.hdfs.server.datanode.DataXceiver@24b42c99] INFO  datanode.DataNode (DataXceiver.java:run(291)) - Failure Recovery isShadow is 289true
2024-03-05 22:39:16,145 [Thread-144] INFO  hdfs.DataStreamer (ShadowDataStreamer.java:createBlockOutputStream(2477)) - ShadowDataStreamer: 2496
2024-03-05 22:39:16,146 [DataXceiver for client /127.0.0.1:40976 [Waiting for operation #1]] INFO  datanode.DataNode (DataXceiver.java:run(307)) - Failure Recovery isShadow is beforereadOptrue
2024-03-05 22:39:16,145 [DataXceiver for client /127.0.0.1:50378 [Waiting for operation #1]] INFO  datanode.DataNode (DataXceiver.java:run(339)) - Failure Recovery isShadow is 333false
Firstbyte is 177
2024-03-05 22:39:16,146 [Thread-144] INFO  datatransfer.DataTransferProtocol (Sender.java:writeBlock(214)) - Failure Recovery Sender before 0true
Failure Recovery convert 788 0
Failure Recovery convert 816 0
2024-03-05 22:39:16,147 [Thread-144] INFO  datatransfer.DataTransferProtocol (Sender.java:writeBlock(219)) - Failure Recovery Sender before 1true
2024-03-05 22:39:16,147 [DataXceiver for client DFSClient_NONMAPREDUCE_-1963227005_1 at /127.0.0.1:50378 [Receiving block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(805)) - Receiving BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001 src: /127.0.0.1:50378 dest: /127.0.0.1:36261
2024-03-05 22:39:16,147 [Thread-144] INFO  datatransfer.DataTransferProtocol (Sender.java:writeBlock(223)) - Failure Recovery Sender before 2true
2024-03-05 22:39:16,147 [DataXceiver for client DFSClient_NONMAPREDUCE_-1963227005_1 at /127.0.0.1:50378 [Receiving block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001]] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:recoverRbw(1663)) - Recover RBW replica BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001
2024-03-05 22:39:16,147 [Thread-144] INFO  datatransfer.DataTransferProtocol (Sender.java:writeBlock(242)) - Failure Recovery Sender before 3true
2024-03-05 22:39:16,147 [Thread-144] INFO  datatransfer.DataTransferProtocol (Sender.java:writeBlock(250)) - Failure Recovery Sender before 6true
2024-03-05 22:39:16,147 [Thread-144] INFO  datatransfer.DataTransferProtocol (Sender.java:send(95)) - Failure Recovery send 7 true
2024-03-05 22:39:16,147 [Thread-144] INFO  datatransfer.DataTransferProtocol (Sender.java:send(97)) - Failure Recovery send 8 true
2024-03-05 22:39:16,147 [DataXceiver for client DFSClient_NONMAPREDUCE_-1963227005_1 at /127.0.0.1:50378 [Receiving block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001]] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:recoverRbw(1681)) - At 127.0.0.1:36261, Recovering ReplicaBeingWritten, blk_1073741825_1001, RBW
  getNumBytes()     = 2
  getBytesOnDisk()  = 2
  getVisibleLength()= 1
  getVolume()       = /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
  getBlockURI()     = file:/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1607339391-127.0.1.1-1709696352182/current/rbw/blk_1073741825
  bytesAcked=1
  bytesOnDisk=2
2024-03-05 22:39:16,147 [Thread-144] INFO  datatransfer.DataTransferProtocol (Sender.java:send(99)) - Failure Recovery send 9 true
2024-03-05 22:39:16,147 [DataXceiver for client DFSClient_NONMAPREDUCE_-1963227005_1 at /127.0.0.1:50378 [Receiving block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001]] INFO  datanode.LocalReplica (LocalReplica.java:truncateBlock(473)) - truncateBlock: blockFile=/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1607339391-127.0.1.1-1709696352182/current/rbw/blk_1073741825, metaFile=/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1607339391-127.0.1.1-1709696352182/current/rbw/blk_1073741825_1001.meta, oldlen=2, newlen=1
2024-03-05 22:39:16,148 [Thread-144] INFO  datatransfer.DataTransferProtocol (Sender.java:send(101)) - Failure Recovery send 10 true
2024-03-05 22:39:16,148 [DataXceiver for client /127.0.0.1:40976 [Waiting for operation #1]] INFO  datanode.DataNode (DataXceiver.java:run(309)) - Failure Recovery isShadow is aftereadOptrue
2024-03-05 22:39:16,148 [DataXceiver for client /127.0.0.1:40976 [Waiting for operation #1]] INFO  datanode.DataNode (DataXceiver.java:run(324)) - Failure Recovery isShadow is 322true
2024-03-05 22:39:16,148 [DataXceiver for client /127.0.0.1:40976 [Waiting for operation #1]] INFO  datanode.DataNode (DataXceiver.java:run(331)) - Failure Recovery isShadow is 329true
2024-03-05 22:39:16,148 [DataXceiver for client /127.0.0.1:40976 [Waiting for operation #1]] INFO  datanode.DataNode (DataXceiver.java:run(339)) - Failure Recovery isShadow is 333true
Firstbyte is 177
2024-03-05 22:39:16,148 [Thread-144] INFO  datatransfer.DataTransferProtocol (Sender.java:writeBlock(254)) - Failure Recovery Sender aftertrue
Failure Recovery convert 788 0
2024-03-05 22:39:16,149 [Thread-144] INFO  hdfs.DataStreamer (ShadowDataStreamer.java:createBlockOutputStream(2486)) - ShadowDataStreamer: 2505
Failure Recovery convert 816 0
2024-03-05 22:39:16,149 [DataXceiver for client DFSClient_NONMAPREDUCE_-1963227005_1 at /127.0.0.1:40976 [Shadow Receiving block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001]] INFO  datanode.DataNode (DataXceiver.java:shadowWriteBlock(1072)) - ShadowWriteBlock 1068
2024-03-05 22:39:16,149 [DataXceiver for client DFSClient_NONMAPREDUCE_-1963227005_1 at /127.0.0.1:40976 [Shadow Receiving block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001]] INFO  datanode.DataNode (DataXceiver.java:shadowWriteBlock(1077)) - ShadowWriteBlock 1073
2024-03-05 22:39:16,149 [DataXceiver for client DFSClient_NONMAPREDUCE_-1963227005_1 at /127.0.0.1:40976 [Shadow Receiving block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001]] INFO  datanode.DataNode (DataXceiver.java:shadowWriteBlock(1085)) - ShadowWriteBlock 1081
2024-03-05 22:39:16,149 [DataXceiver for client DFSClient_NONMAPREDUCE_-1963227005_1 at /127.0.0.1:40976 [Shadow Receiving block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001]] INFO  datanode.DataNode (DataXceiver.java:shadowWriteBlock(1107)) - ShadowWriteBlock Receiving BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001 src: /127.0.0.1:40976 dest: /127.0.0.1:45427
2024-03-05 22:39:16,149 [DataXceiver for client DFSClient_NONMAPREDUCE_-1963227005_1 at /127.0.0.1:40976 [Shadow Receiving block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001]] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:recoverRbw(1663)) - Recover RBW replica BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001
2024-03-05 22:39:16,149 [DataXceiver for client DFSClient_NONMAPREDUCE_-1963227005_1 at /127.0.0.1:50378 [Receiving block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(960)) - WriteBlock mirrorInStatus is SUCCESS, firstBadLink is 
Firstbyte is 4
2024-03-05 22:39:16,150 [DataXceiver for client DFSClient_NONMAPREDUCE_-1963227005_1 at /127.0.0.1:50378 [Receiving block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(1000)) - opWriteBlock BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1002 received exception java.nio.channels.ClosedByInterruptException
2024-03-05 22:39:16,150 [DataXceiver for client DFSClient_NONMAPREDUCE_-1963227005_1 at /127.0.0.1:50378 [Receiving block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001]] ERROR datanode.DataNode (DataXceiver.java:run(373)) - 127.0.0.1:36261:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:50378 dst: /127.0.0.1:36261
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:477)
	at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:62)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:141)
	at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:158)
	at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:116)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
	at java.io.DataOutputStream.flush(DataOutputStream.java:123)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:961)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:219)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:110)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:340)
	at java.lang.Thread.run(Thread.java:750)
2024-03-05 22:39:16,151 [DataXceiver for client DFSClient_NONMAPREDUCE_-1963227005_1 at /127.0.0.1:40976 [Shadow Receiving block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001]] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:recoverRbw(1681)) - At 127.0.0.1:36261, Recovering ReplicaBeingWritten, blk_1073741825_1002, RBW
  getNumBytes()     = 1
  getBytesOnDisk()  = 1
  getVisibleLength()= 1
  getVolume()       = /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
  getBlockURI()     = file:/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1607339391-127.0.1.1-1709696352182/current/rbw/blk_1073741825
  bytesAcked=1
  bytesOnDisk=1
2024-03-05 22:39:16,151 [DataXceiver for client DFSClient_NONMAPREDUCE_-1963227005_1 at /127.0.0.1:40976 [Shadow Receiving block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001]] INFO  datanode.DataNode (DataXceiver.java:shadowWriteBlock(1137)) - ShadowWriteBlock 1133
2024-03-05 22:39:16,151 [DataXceiver for client DFSClient_NONMAPREDUCE_-1963227005_1 at /127.0.0.1:40976 [Shadow Receiving block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001]] INFO  datanode.DataNode (DataXceiver.java:shadowWriteBlock(1254)) - ShadowWriteBlock 1252
2024-03-05 22:39:16,151 [DataXceiver for client DFSClient_NONMAPREDUCE_-1963227005_1 at /127.0.0.1:40976 [Shadow Receiving block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001]] INFO  datanode.DataNode (DataXceiver.java:shadowWriteBlock(1266)) - ShadowWriteBlock mirrorInStatus is SUCCESS, firstBadLink is 
2024-03-05 22:39:16,151 [DataXceiver for client DFSClient_NONMAPREDUCE_-1963227005_1 at /127.0.0.1:40976 [Shadow Receiving block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001]] INFO  datanode.DataNode (DataXceiver.java:shadowWriteBlock(1271)) - ShadowWriteBlock 1268
Firstbyte is 4
2024-03-05 22:39:16,151 [Thread-144] INFO  hdfs.DataStreamer (ShadowDataStreamer.java:setupPipelineInternal(2138)) - ShadowDataStreamer: create pipeline success
dn.shadowIpaddr is127.0.0.1
dn.shadowIpaddr is127.0.0.1
dn.shadowXferPort is45427
dn.shadowXferPort is45427
2024-03-05 22:39:16,157 [IPC Server handler 8 on default port 36175] INFO  namenode.FSNamesystem (FSNamesystem.java:updatePipeline(6037)) - updatePipeline(blk_1073741825_1001, newGS=1002, newLength=1, newNodes=[127.0.0.1:36261], client=DFSClient_NONMAPREDUCE_-1963227005_1)
2024-03-05 22:39:16,157 [IPC Server handler 6 on default port 36175] INFO  namenode.FSNamesystem (FSNamesystem.java:updatePipeline(6037)) - updatePipeline(blk_1073741825_1001, newGS=1003, newLength=1, newNodes=[127.0.0.1:36261], client=DFSClient_NONMAPREDUCE_-1963227005_1)
2024-03-05 22:39:16,157 [IPC Server handler 8 on default port 36175] INFO  namenode.FSNamesystem (FSNamesystem.java:updatePipeline(6055)) - updatePipeline(blk_1073741825_1001 => blk_1073741825_1002) success
2024-03-05 22:39:16,157 [IPC Server handler 6 on default port 36175] INFO  namenode.FSNamesystem (FSNamesystem.java:updatePipeline(6055)) - updatePipeline(blk_1073741825_1001 => blk_1073741825_1003) success
2024-03-05 22:39:16,159 [DataStreamer for file /user/zhenyu/noheartbeat.dat block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1002] INFO  hdfs.DataStreamer (DataStreamer.java:run(777)) - Before shadowErrorHandler, the nodes are: [DatanodeInfoWithStorage[127.0.0.1:36261,DS-a0953621-3236-40ef-8e69-cbc77fe0dd76,DISK]]
2024-03-05 22:39:16,159 [ResponseProcessor for block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1002] WARN  hdfs.DataStreamer (DataStreamer.java:run(1375)) - Exception for BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1002
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:538)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:244)
	at org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1251)
2024-03-05 22:39:16,159 [DataStreamer for file /user/zhenyu/noheartbeat.dat block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1002] INFO  hdfs.DataStreamer (DataStreamer.java:processDatanodeOrExternalError(1529)) - Error Recovery for BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1002 waiting for responder to exit. 
2024-03-05 22:39:16,162 [DataXceiver for client DFSClient_NONMAPREDUCE_-1963227005_1 at /127.0.0.1:40976 [Shadow Receiving block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001]] INFO  datanode.DataNode (BlockReceiver.java:receiveBlock(1538)) - Exception for BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1003
java.io.IOException: Premature EOF from inputStream
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:216)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:221)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:144)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:119)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:705)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:1499)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.shadowWriteBlock(DataXceiver.java:1276)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:741)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:219)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:110)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:340)
	at java.lang.Thread.run(Thread.java:750)
2024-03-05 22:39:16,162 [PacketResponder: BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1003, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(2003)) - PacketResponder: BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1003, type=LAST_IN_PIPELINE: Thread is interrupted.
2024-03-05 22:39:16,162 [PacketResponder: BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1003, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(2045)) - PacketResponder: BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1003, type=LAST_IN_PIPELINE terminating
2024-03-05 22:39:16,162 [DataXceiver for client DFSClient_NONMAPREDUCE_-1963227005_1 at /127.0.0.1:40976 [Shadow Receiving block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001]] INFO  datanode.DataNode (DataXceiver.java:shadowWriteBlock(1312)) - opWriteBlock BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1003 received exception java.io.IOException: Premature EOF from inputStream
2024-03-05 22:39:16,162 [DataXceiver for client DFSClient_NONMAPREDUCE_-1963227005_1 at /127.0.0.1:40976 [Shadow Receiving block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1001]] ERROR datanode.DataNode (DataXceiver.java:run(373)) - 127.0.0.1:36261:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:40976 dst: /127.0.0.1:45427
java.io.IOException: Premature EOF from inputStream
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:216)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:221)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:144)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:119)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:705)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:1499)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.shadowWriteBlock(DataXceiver.java:1276)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:741)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:219)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:110)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:340)
	at java.lang.Thread.run(Thread.java:750)
dn.shadowIpaddr is127.0.0.1
dn.shadowIpaddr is127.0.0.1
dn.shadowXferPort is39543
dn.shadowXferPort is45427
dn.shadowIpaddr is127.0.0.1
dn.shadowIpaddr is127.0.0.1
dn.shadowXferPort is45427
dn.shadowXferPort is39543
2024-03-05 22:39:17,659 [DataStreamer for file /user/zhenyu/noheartbeat.dat block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1002] INFO  hdfs.DataStreamer (DataStreamer.java:run(777)) - Before shadowErrorHandler, the nodes are: [DatanodeInfoWithStorage[127.0.0.1:36261,DS-a0953621-3236-40ef-8e69-cbc77fe0dd76,DISK]]
2024-03-05 22:39:17,659 [DataStreamer for file /user/zhenyu/noheartbeat.dat block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1002] INFO  hdfs.DataStreamer (DataStreamer.java:handleBadDatanode(2102)) - Failure Recovery: prepare For Processing 0
2024-03-05 22:39:19,659 [DataStreamer for file /user/zhenyu/noheartbeat.dat block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1002] INFO  hdfs.DataStreamer (ShadowDataStreamer.java:prepareForProcessing(591)) - ShadowDataStreamer: prepare For Processing
2024-03-05 22:39:19,659 [DataStreamer for file /user/zhenyu/noheartbeat.dat block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1002] INFO  hdfs.DataStreamer (ShadowDataStreamer.java:copyFromDataStreamer(561)) - Failure Recovery: prepare For Processing
2024-03-05 22:39:19,659 [DataStreamer for file /user/zhenyu/noheartbeat.dat block BP-1607339391-127.0.1.1-1709696352182:blk_1073741825_1002] INFO  hdfs.DataStreamer (ShadowDataStreamer.java:copyFromDataStreamer(564)) - ShadowDataStreamer: ErrorState: 0
2024-03-05 22:39:19,659 [main] WARN  hdfs.DFSClient (DFSOutputStream.java:flushOrSync(765)) - Error while syncing
java.io.IOException: All datanodes [DatanodeInfoWithStorage[127.0.0.1:36261,DS-a0953621-3236-40ef-8e69-cbc77fe0dd76,DISK]] are bad. Aborting...
	at org.apache.hadoop.hdfs.DataStreamer.handleBadDatanode(DataStreamer.java:2112)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1975)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1923)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1558)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:778)
2024-03-05 22:39:19,660 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(2182)) - Shutting down the Mini HDFS Cluster
2024-03-05 22:39:19,661 [main] ERROR hdfs.DFSClient (DFSClient.java:closeAllFilesBeingWritten(665)) - Failed to close file: /user/zhenyu/noheartbeat.dat with renewLeaseKey: DEFAULT_16388
java.io.IOException: All datanodes [DatanodeInfoWithStorage[127.0.0.1:36261,DS-a0953621-3236-40ef-8e69-cbc77fe0dd76,DISK]] are bad. Aborting...
	at org.apache.hadoop.hdfs.DataStreamer.handleBadDatanode(DataStreamer.java:2112)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1975)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1923)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1558)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:778)
2024-03-05 22:39:19,661 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2231)) - Shutting down DataNode 1
2024-03-05 22:39:19,662 [main] INFO  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(431)) - Shutdown has been called
2024-03-05 22:39:19,662 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@37bd68c3] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(444)) - Closing all peers.
2024-03-05 22:39:19,663 [VolumeScannerThread(/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(672)) - VolumeScanner(/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, DS-dbb30921-c74f-4995-8ef2-b17475f34882) exiting.
2024-03-05 22:39:19,663 [VolumeScannerThread(/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(672)) - VolumeScanner(/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3, DS-19547c29-546b-4b55-86e8-03fd99bb066e) exiting.
2024-03-05 22:39:19,693 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.w.WebAppContext@71b3bc45{datanode,/,null,STOPPED}{file:/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode}
2024-03-05 22:39:19,699 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStop(383)) - Stopped ServerConnector@239b0f9d{HTTP/1.1, (http/1.1)}{localhost:0}
2024-03-05 22:39:19,700 [main] INFO  server.session (HouseKeeper.java:stopScavenging(149)) - node0 Stopped scavenging
2024-03-05 22:39:19,700 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.s.ServletContextHandler@5827af16{static,/static,file:///home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,STOPPED}
2024-03-05 22:39:19,701 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.s.ServletContextHandler@4bff1903{logs,/logs,file:///home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/log/,STOPPED}
2024-03-05 22:39:19,706 [main] INFO  datanode.DataNode (DataNode.java:shutdown(2614)) - Waiting up to 30 seconds for transfer threads to complete
2024-03-05 22:39:19,706 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@60f7cc1d] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(444)) - Closing all peers.
2024-03-05 22:39:19,706 [main] INFO  ipc.Server (Server.java:stop(3704)) - Stopping server on 35077
2024-03-05 22:39:19,707 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1587)) - Stopping IPC Server listener on 0
2024-03-05 22:39:19,707 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1720)) - Stopping IPC Server Responder
2024-03-05 22:39:19,707 [Command processor] ERROR datanode.DataNode (BPServiceActor.java:processQueue(1420)) - Command processor encountered interrupt and exit.
2024-03-05 22:39:19,707 [BP-1607339391-127.0.1.1-1709696352182 heartbeating to localhost/127.0.0.1:36175] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2024-03-05 22:39:19,708 [Command processor] WARN  datanode.DataNode (BPServiceActor.java:run(1404)) - Ending command processor service for: Thread[Command processor,5,main]
2024-03-05 22:39:19,708 [BP-1607339391-127.0.1.1-1709696352182 heartbeating to localhost/127.0.0.1:36175] WARN  datanode.DataNode (BPServiceActor.java:run(929)) - Ending block pool service for: Block pool BP-1607339391-127.0.1.1-1709696352182 (Datanode Uuid e0db7eb1-f861-4de4-a0df-2a2a3acfcfcd) service to localhost/127.0.0.1:36175
2024-03-05 22:39:19,708 [BP-1607339391-127.0.1.1-1709696352182 heartbeating to localhost/127.0.0.1:36175] INFO  datanode.DataNode (BlockPoolManager.java:remove(103)) - Removed Block pool BP-1607339391-127.0.1.1-1709696352182 (Datanode Uuid e0db7eb1-f861-4de4-a0df-2a2a3acfcfcd)
2024-03-05 22:39:19,708 [BP-1607339391-127.0.1.1-1709696352182 heartbeating to localhost/127.0.0.1:36175] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(3256)) - Removing block pool BP-1607339391-127.0.1.1-1709696352182
2024-03-05 22:39:19,709 [refreshUsed-/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1607339391-127.0.1.1-1709696352182] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(231)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2024-03-05 22:39:19,709 [refreshUsed-/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1607339391-127.0.1.1-1709696352182] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(231)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2024-03-05 22:39:19,710 [main] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(212)) - Shutting down all async disk service threads
2024-03-05 22:39:19,710 [main] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(220)) - All async disk service threads have been shut down
2024-03-05 22:39:19,710 [main] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(186)) - Shutting down all async lazy persist service threads
2024-03-05 22:39:19,710 [main] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(193)) - All async lazy persist service threads have been shut down
2024-03-05 22:39:19,711 [main] INFO  datanode.DataNode (DataNode.java:shutdown(2705)) - Shutdown complete.
2024-03-05 22:39:19,711 [main] WARN  datanode.DataSetLockManager (DataSetLockManager.java:lockLeakCheck(261)) - not open lock leak check func
2024-03-05 22:39:19,711 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2231)) - Shutting down DataNode 0
2024-03-05 22:39:19,711 [main] INFO  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(431)) - Shutdown has been called
2024-03-05 22:39:19,711 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@62ef27a8] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(444)) - Closing all peers.
2024-03-05 22:39:19,712 [VolumeScannerThread(/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(672)) - VolumeScanner(/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-a0953621-3236-40ef-8e69-cbc77fe0dd76) exiting.
2024-03-05 22:39:19,712 [VolumeScannerThread(/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(672)) - VolumeScanner(/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-fb698f9f-eda6-4ec4-8154-78ab02d10859) exiting.
2024-03-05 22:39:19,736 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.w.WebAppContext@4b14918a{datanode,/,null,STOPPED}{file:/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode}
2024-03-05 22:39:19,737 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStop(383)) - Stopped ServerConnector@4b1d6571{HTTP/1.1, (http/1.1)}{localhost:0}
2024-03-05 22:39:19,737 [main] INFO  server.session (HouseKeeper.java:stopScavenging(149)) - node0 Stopped scavenging
2024-03-05 22:39:19,738 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.s.ServletContextHandler@6995bf68{static,/static,file:///home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,STOPPED}
2024-03-05 22:39:19,739 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.s.ServletContextHandler@503ecb24{logs,/logs,file:///home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/log/,STOPPED}
2024-03-05 22:39:19,740 [main] INFO  datanode.DataNode (DataNode.java:shutdown(2614)) - Waiting up to 30 seconds for transfer threads to complete
2024-03-05 22:39:19,740 [main] INFO  ipc.Server (Server.java:stop(3704)) - Stopping server on 37983
2024-03-05 22:39:19,740 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@6436a7db] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(444)) - Closing all peers.
2024-03-05 22:39:19,741 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1587)) - Stopping IPC Server listener on 0
2024-03-05 22:39:19,741 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1720)) - Stopping IPC Server Responder
2024-03-05 22:39:19,741 [Command processor] ERROR datanode.DataNode (BPServiceActor.java:processQueue(1420)) - Command processor encountered interrupt and exit.
2024-03-05 22:39:19,741 [BP-1607339391-127.0.1.1-1709696352182 heartbeating to localhost/127.0.0.1:36175] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2024-03-05 22:39:19,741 [Command processor] WARN  datanode.DataNode (BPServiceActor.java:run(1404)) - Ending command processor service for: Thread[Command processor,5,main]
2024-03-05 22:39:19,741 [BP-1607339391-127.0.1.1-1709696352182 heartbeating to localhost/127.0.0.1:36175] WARN  datanode.DataNode (BPServiceActor.java:run(929)) - Ending block pool service for: Block pool BP-1607339391-127.0.1.1-1709696352182 (Datanode Uuid e4a85aef-13ce-4b47-8b3c-6054f58e20ea) service to localhost/127.0.0.1:36175
2024-03-05 22:39:19,743 [BP-1607339391-127.0.1.1-1709696352182 heartbeating to localhost/127.0.0.1:36175] INFO  datanode.DataNode (BlockPoolManager.java:remove(103)) - Removed Block pool BP-1607339391-127.0.1.1-1709696352182 (Datanode Uuid e4a85aef-13ce-4b47-8b3c-6054f58e20ea)
2024-03-05 22:39:19,743 [BP-1607339391-127.0.1.1-1709696352182 heartbeating to localhost/127.0.0.1:36175] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(3256)) - Removing block pool BP-1607339391-127.0.1.1-1709696352182
2024-03-05 22:39:19,744 [refreshUsed-/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1607339391-127.0.1.1-1709696352182] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(231)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2024-03-05 22:39:19,744 [refreshUsed-/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1607339391-127.0.1.1-1709696352182] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(231)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2024-03-05 22:39:19,744 [main] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(212)) - Shutting down all async disk service threads
2024-03-05 22:39:19,745 [main] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(220)) - All async disk service threads have been shut down
2024-03-05 22:39:19,745 [main] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(186)) - Shutting down all async lazy persist service threads
2024-03-05 22:39:19,745 [main] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(193)) - All async lazy persist service threads have been shut down
2024-03-05 22:39:19,746 [main] INFO  datanode.DataNode (DataNode.java:shutdown(2705)) - Shutdown complete.
2024-03-05 22:39:19,746 [main] WARN  datanode.DataSetLockManager (DataSetLockManager.java:lockLeakCheck(261)) - not open lock leak check func
2024-03-05 22:39:19,746 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopAndJoinNameNode(2264)) - Shutting down the namenode
2024-03-05 22:39:19,747 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1509)) - Stopping services started for active state
2024-03-05 22:39:19,747 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@34158c08] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4696)) - LazyPersistFileScrubber was interrupted, exiting
2024-03-05 22:39:19,747 [main] INFO  namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1467)) - Ending log segment 1, 12
2024-03-05 22:39:19,748 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@2f16c6b3] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4599)) - NameNodeEditLogRoller was interrupted, exiting
2024-03-05 22:39:19,748 [main] INFO  namenode.FSEditLog (FSEditLog.java:printStatistics(801)) - Number of transactions: 13 Total time for transactions(ms): 10 Number of transactions batched in Syncs: 1 Number of syncs: 13 SyncTimes(ms): 1 0 
2024-03-05 22:39:19,751 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(145)) - Finalizing edits file /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_inprogress_0000000000000000001 -> /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000001-0000000000000000013
2024-03-05 22:39:19,752 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(145)) - Finalizing edits file /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_inprogress_0000000000000000001 -> /home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000001-0000000000000000013
2024-03-05 22:39:19,753 [FSEditLogAsync] INFO  namenode.FSEditLog (FSEditLogAsync.java:run(276)) - FSEditLogAsync was interrupted, exiting
2024-03-05 22:39:19,753 [CacheReplicationMonitor(378060006)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(174)) - Shutting down CacheReplicationMonitor
2024-03-05 22:39:19,755 [main] INFO  ipc.Server (Server.java:stop(3704)) - Stopping server on 36175
2024-03-05 22:39:19,755 [IPC Server listener on 0] INFO  ipc.Server (Server.java:run(1587)) - Stopping IPC Server listener on 0
2024-03-05 22:39:19,755 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1720)) - Stopping IPC Server Responder
2024-03-05 22:39:19,756 [MarkedDeleteBlockScrubberThread] INFO  blockmanagement.BlockManager (BlockManager.java:run(5332)) - Stopping MarkedDeleteBlockScrubber.
2024-03-05 22:39:19,756 [RedundancyMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(5359)) - Stopping RedundancyMonitor.
2024-03-05 22:39:19,756 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:stopSlowPeerCollector(412)) - Slow peers collection thread shutdown
2024-03-05 22:39:19,768 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1509)) - Stopping services started for active state
2024-03-05 22:39:19,769 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1613)) - Stopping services started for standby state
2024-03-05 22:39:19,770 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.w.WebAppContext@6f7923a5{hdfs,/,null,STOPPED}{file:/home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs}
2024-03-05 22:39:19,772 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStop(383)) - Stopped ServerConnector@37911f88{HTTP/1.1, (http/1.1)}{localhost:0}
2024-03-05 22:39:19,773 [main] INFO  server.session (HouseKeeper.java:stopScavenging(149)) - node0 Stopped scavenging
2024-03-05 22:39:19,773 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.s.ServletContextHandler@536dbea0{static,/static,file:///home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,STOPPED}
2024-03-05 22:39:19,774 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(1159)) - Stopped o.e.j.s.ServletContextHandler@4bff7da0{logs,/logs,file:///home/zhenyu/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/log/,STOPPED}
2024-03-05 22:39:19,776 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2024-03-05 22:39:19,777 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2024-03-05 22:39:19,778 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(612)) - DataNode metrics system shutdown complete.
